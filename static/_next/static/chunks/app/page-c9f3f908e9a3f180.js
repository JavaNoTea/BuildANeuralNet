(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[974],{2323:(e,t,a)=>{"use strict";a.d(t,{n:()=>r});var n=a(397),i=a(7287);let r=(0,n.v)()((0,i.Zr)(e=>({user:null,accessToken:null,refreshToken:null,isAuthenticated:!1,isLoading:!1,setAuth:(t,a,n)=>e({user:t,accessToken:a,refreshToken:n,isAuthenticated:!0}),setTokens:(t,a)=>e({accessToken:t,refreshToken:a}),logout:()=>e({user:null,accessToken:null,refreshToken:null,isAuthenticated:!1}),updateTokens:(t,a)=>e({accessToken:t,refreshToken:a}),setLoading:t=>e({isLoading:t})}),{name:"auth-storage",partialize:e=>({accessToken:e.accessToken,refreshToken:e.refreshToken})}))},2480:(e,t,a)=>{"use strict";a.r(t),a.d(t,{default:()=>ee});var n=a(6384),i=a(6636);let r=[{category:"Input",friendly:"Dataset",torchClass:"Dataset",description:"Input dataset node providing data to the neural network. Represents the entry point for training and inference data. Connect this to the first layer of your model.",defaults:{},params:[]},{category:"Layers",friendly:"Linear",torchClass:"torch.nn.Linear",description:"Fully connected linear layer performing affine transformation: y = xW^T + b. Essential building block for feedforward networks, classification heads, and dense connections.",defaults:{in_features:128,out_features:64,bias:!0},params:[{name:"in_features",type:"int",min:1,required:!0},{name:"out_features",type:"int",min:1,required:!0},{name:"bias",type:"bool"}]},{category:"Layers",friendly:"Conv1d",torchClass:"torch.nn.Conv1d",description:"1D convolution for sequential data like audio signals or time series. Applies convolution operation along one spatial dimension. Useful for signal processing and 1D pattern recognition.",defaults:{in_channels:1,out_channels:32,kernel_size:3,stride:1,padding:0,dilation:1,groups:1,bias:!0,padding_mode:"zeros"},params:[{name:"in_channels",type:"int",min:1,required:!0},{name:"out_channels",type:"int",min:1,required:!0},{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"dilation",type:"int",min:1},{name:"groups",type:"int",min:1},{name:"bias",type:"bool"},{name:"padding_mode",type:"select",options:["zeros","reflect","replicate","circular"]}]},{category:"Layers",friendly:"Conv2d",torchClass:"torch.nn.Conv2d",description:"2D convolution for image processing. Applies 2D convolution over input signal with learnable filters. Core component of CNNs for feature extraction from spatial data.",defaults:{in_channels:3,out_channels:32,kernel_size:3,stride:1,padding:0,dilation:1,groups:1,bias:!0,padding_mode:"zeros"},params:[{name:"in_channels",type:"int",min:1,required:!0},{name:"out_channels",type:"int",min:1,required:!0},{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"dilation",type:"int",min:1},{name:"groups",type:"int",min:1},{name:"bias",type:"bool"},{name:"padding_mode",type:"select",options:["zeros","reflect","replicate","circular"]}]},{category:"Layers",friendly:"Conv3d",torchClass:"torch.nn.Conv3d",description:"3D convolution for volumetric data like videos or medical scans. Applies 3D convolution over input signal. Useful for temporal-spatial feature extraction.",defaults:{in_channels:3,out_channels:32,kernel_size:3,stride:1,padding:0,dilation:1,groups:1,bias:!0,padding_mode:"zeros"},params:[{name:"in_channels",type:"int",min:1,required:!0},{name:"out_channels",type:"int",min:1,required:!0},{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"dilation",type:"int",min:1},{name:"groups",type:"int",min:1},{name:"bias",type:"bool"},{name:"padding_mode",type:"select",options:["zeros","reflect","replicate","circular"]}]},{category:"Layers",friendly:"Embedding",torchClass:"torch.nn.Embedding",description:"Embedding layer that maps discrete tokens to dense vector representations. Essential for NLP tasks to convert words/tokens into learnable dense vectors. Vocabulary size determines num_embeddings.",defaults:{num_embeddings:1e3,embedding_dim:128,padding_idx:null,max_norm:null,norm_type:2,scale_grad_by_freq:!1,sparse:!1},params:[{name:"num_embeddings",type:"int",min:1,required:!0},{name:"embedding_dim",type:"int",min:1,required:!0},{name:"padding_idx",type:"int",min:0},{name:"max_norm",type:"float",min:0},{name:"norm_type",type:"float",min:0},{name:"scale_grad_by_freq",type:"bool"},{name:"sparse",type:"bool"}]},{category:"Layers",friendly:"LSTM",torchClass:"torch.nn.LSTM",description:"Long Short-Term Memory recurrent layer for sequential data. Excellent for handling long-term dependencies in sequences. Use for NLP, time series prediction, and sequential pattern recognition.",defaults:{input_size:128,hidden_size:64,num_layers:1,bias:!0,batch_first:!1,dropout:0,bidirectional:!1},params:[{name:"input_size",type:"int",min:1,required:!0},{name:"hidden_size",type:"int",min:1,required:!0},{name:"num_layers",type:"int",min:1},{name:"bias",type:"bool"},{name:"batch_first",type:"bool"},{name:"dropout",type:"float",min:0,max:1},{name:"bidirectional",type:"bool"}]},{category:"Layers",friendly:"GRU",torchClass:"torch.nn.GRU",description:"Gated Recurrent Unit for sequential data processing. Simpler alternative to LSTM with fewer gates. Good balance between computational efficiency and sequence modeling capability.",defaults:{input_size:128,hidden_size:64,num_layers:1,bias:!0,batch_first:!1,dropout:0,bidirectional:!1},params:[{name:"input_size",type:"int",min:1,required:!0},{name:"hidden_size",type:"int",min:1,required:!0},{name:"num_layers",type:"int",min:1},{name:"bias",type:"bool"},{name:"batch_first",type:"bool"},{name:"dropout",type:"float",min:0,max:1},{name:"bidirectional",type:"bool"}]},{category:"Layers",friendly:"RNN",torchClass:"torch.nn.RNN",description:"Basic Recurrent Neural Network layer for sequential data. Simple RNN with tanh or ReLU activation. Foundation for understanding recurrent architectures.",defaults:{input_size:128,hidden_size:64,num_layers:1,nonlinearity:"tanh",bias:!0,batch_first:!1,dropout:0,bidirectional:!1},params:[{name:"input_size",type:"int",min:1,required:!0},{name:"hidden_size",type:"int",min:1,required:!0},{name:"num_layers",type:"int",min:1},{name:"nonlinearity",type:"select",options:["tanh","relu"]},{name:"bias",type:"bool"},{name:"batch_first",type:"bool"},{name:"dropout",type:"float",min:0,max:1},{name:"bidirectional",type:"bool"}]},{category:"Layers",friendly:"Flatten",torchClass:"torch.nn.Flatten",description:"Flattens input tensor into 1D while preserving batch dimension. Essential for transitioning from convolutional layers to fully connected layers in CNN architectures.",defaults:{start_dim:1,end_dim:-1},params:[{name:"start_dim",type:"int"},{name:"end_dim",type:"int"}]},{category:"Layers",friendly:"ConvTranspose2d",torchClass:"torch.nn.ConvTranspose2d",description:"Transposed 2D convolution (deconvolution) for upsampling. Increases spatial dimensions. Essential for decoder networks, GANs, and segmentation models.",defaults:{in_channels:64,out_channels:32,kernel_size:3,stride:1,padding:0,output_padding:0,groups:1,bias:!0,dilation:1,padding_mode:"zeros"},params:[{name:"in_channels",type:"int",min:1,required:!0},{name:"out_channels",type:"int",min:1,required:!0},{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"output_padding",type:"int",min:0},{name:"groups",type:"int",min:1},{name:"bias",type:"bool"},{name:"dilation",type:"int",min:1},{name:"padding_mode",type:"select",options:["zeros","reflect","replicate","circular"]}]},{category:"Activations",friendly:"ReLU",torchClass:"torch.nn.ReLU",description:"Rectified Linear Unit: max(0, x). Most widely used activation function. Solves vanishing gradient problem and provides computational efficiency. Default choice for hidden layers.",defaults:{inplace:!1},params:[{name:"inplace",type:"bool"}]},{category:"Activations",friendly:"LeakyReLU",torchClass:"torch.nn.LeakyReLU",description:"Leaky ReLU: max(0.01x, x). Addresses dying ReLU problem by allowing small negative values. Helps maintain gradient flow for negative inputs.",defaults:{negative_slope:.01,inplace:!1},params:[{name:"negative_slope",type:"float"},{name:"inplace",type:"bool"}]},{category:"Activations",friendly:"ELU",torchClass:"torch.nn.ELU",description:"Exponential Linear Unit: x if x>0, α(exp(x)-1) if x≤0. Smooth activation with mean closer to zero. Reduces bias shift and speeds up learning.",defaults:{alpha:1,inplace:!1},params:[{name:"alpha",type:"float"},{name:"inplace",type:"bool"}]},{category:"Activations",friendly:"SELU",torchClass:"torch.nn.SELU",description:"Scaled Exponential Linear Unit. Self-normalizing activation function. Designed to maintain zero mean and unit variance. Use with proper weight initialization.",defaults:{inplace:!1},params:[{name:"inplace",type:"bool"}]},{category:"Activations",friendly:"GELU",torchClass:"torch.nn.GELU",description:"Gaussian Error Linear Unit. Smooth approximation to ReLU. Popular in transformer architectures. Provides probabilistic interpretation of activation.",defaults:{approximate:"none"},params:[{name:"approximate",type:"select",options:["none","tanh"]}]},{category:"Activations",friendly:"Sigmoid",torchClass:"torch.nn.Sigmoid",description:"Sigmoid activation: 1/(1+exp(-x)). Maps input to (0,1) range. Commonly used for binary classification output and gating mechanisms in RNNs.",defaults:{},params:[]},{category:"Activations",friendly:"Tanh",torchClass:"torch.nn.Tanh",description:"Hyperbolic tangent: (exp(x)-exp(-x))/(exp(x)+exp(-x)). Maps input to (-1,1) range. Zero-centered output, preferred over sigmoid for hidden layers.",defaults:{},params:[]},{category:"Activations",friendly:"Softmax",torchClass:"torch.nn.Softmax",description:"Softmax activation for multi-class classification. Converts logits to probability distribution. Essential for converting raw scores to class probabilities.",defaults:{dim:null},params:[{name:"dim",type:"int"}]},{category:"Activations",friendly:"Softplus",torchClass:"torch.nn.Softplus",description:"Softplus activation: log(1+exp(x)). Smooth approximation to ReLU. Always positive output, useful for modeling positive quantities.",defaults:{beta:1,threshold:20},params:[{name:"beta",type:"float"},{name:"threshold",type:"float"}]},{category:"Activations",friendly:"Softsign",torchClass:"torch.nn.Softsign",description:"Softsign activation: x/(1+|x|). Maps input to (-1,1) range. Polynomial convergence vs exponential for tanh. Useful alternative to tanh.",defaults:{},params:[]},{category:"Activations",friendly:"Hardtanh",torchClass:"torch.nn.Hardtanh",description:"Hard tanh activation: linear clipping. Computationally efficient alternative to tanh. Provides bounded output with linear regions.",defaults:{min_val:-1,max_val:1,inplace:!1},params:[{name:"min_val",type:"float"},{name:"max_val",type:"float"},{name:"inplace",type:"bool"}]},{category:"Pooling",friendly:"MaxPool1d",torchClass:"torch.nn.MaxPool1d",description:"1D max pooling reduces spatial dimensions by taking maximum value in each pooling window. Provides translation invariance and reduces computational load. Use after Conv1d layers.",defaults:{kernel_size:2,stride:null,padding:0,dilation:1,return_indices:!1,ceil_mode:!1},params:[{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"dilation",type:"int",min:1},{name:"return_indices",type:"bool"},{name:"ceil_mode",type:"bool"}]},{category:"Pooling",friendly:"MaxPool2d",torchClass:"torch.nn.MaxPool2d",description:"2D max pooling for spatial dimension reduction in images. Takes maximum value in each pooling window. Provides translation invariance and computational efficiency.",defaults:{kernel_size:2,stride:null,padding:0,dilation:1,return_indices:!1,ceil_mode:!1},params:[{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"dilation",type:"int",min:1},{name:"return_indices",type:"bool"},{name:"ceil_mode",type:"bool"}]},{category:"Pooling",friendly:"MaxPool3d",torchClass:"torch.nn.MaxPool3d",description:"3D max pooling for volumetric data reduction. Takes maximum value in 3D pooling windows. Used in 3D CNNs for video or medical imaging.",defaults:{kernel_size:2,stride:null,padding:0,dilation:1,return_indices:!1,ceil_mode:!1},params:[{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"dilation",type:"int",min:1},{name:"return_indices",type:"bool"},{name:"ceil_mode",type:"bool"}]},{category:"Pooling",friendly:"AvgPool1d",torchClass:"torch.nn.AvgPool1d",description:"1D average pooling computes mean values in each pooling window. Smoother downsampling compared to max pooling. Preserves overall signal characteristics.",defaults:{kernel_size:2,stride:null,padding:0,ceil_mode:!1,count_include_pad:!0},params:[{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"ceil_mode",type:"bool"},{name:"count_include_pad",type:"bool"}]},{category:"Pooling",friendly:"AvgPool2d",torchClass:"torch.nn.AvgPool2d",description:"2D average pooling computes spatial mean values. Smoother downsampling preserving spatial relationships. Alternative to max pooling for different feature characteristics.",defaults:{kernel_size:2,stride:null,padding:0,ceil_mode:!1,count_include_pad:!0,divisor_override:null},params:[{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"ceil_mode",type:"bool"},{name:"count_include_pad",type:"bool"},{name:"divisor_override",type:"int",min:1}]},{category:"Pooling",friendly:"AvgPool3d",torchClass:"torch.nn.AvgPool3d",description:"3D average pooling for volumetric data. Computes mean in 3D windows. Provides smooth spatial-temporal downsampling for video or 3D medical data.",defaults:{kernel_size:2,stride:null,padding:0,ceil_mode:!1,count_include_pad:!0,divisor_override:null},params:[{name:"kernel_size",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"padding",type:"int",min:0},{name:"ceil_mode",type:"bool"},{name:"count_include_pad",type:"bool"},{name:"divisor_override",type:"int",min:1}]},{category:"Pooling",friendly:"AdaptiveAvgPool2d",torchClass:"torch.nn.AdaptiveAvgPool2d",description:"Adaptive average pooling that outputs fixed-size regardless of input size. Essential for connecting variable-sized feature maps to fixed-size fully connected layers.",defaults:{output_size:1},params:[{name:"output_size",type:"int",min:1,required:!0}]},{category:"Pooling",friendly:"AdaptiveMaxPool2d",torchClass:"torch.nn.AdaptiveMaxPool2d",description:"Adaptive max pooling that outputs fixed-size regardless of input size. Alternative to adaptive average pooling for extracting maximum activations.",defaults:{output_size:1},params:[{name:"output_size",type:"int",min:1,required:!0}]},{category:"Pooling",friendly:"Upsample",torchClass:"torch.nn.Upsample",description:"Upsamples spatial dimensions using interpolation. Alternative to transposed convolution for increasing feature map size. Supports multiple interpolation modes.",defaults:{size:null,scale_factor:2,mode:"nearest",align_corners:null},params:[{name:"scale_factor",type:"float",min:1},{name:"mode",type:"select",options:["nearest","linear","bilinear","bicubic","trilinear"]},{name:"align_corners",type:"bool"}]},{category:"Normalization",friendly:"BatchNorm1d",torchClass:"torch.nn.BatchNorm1d",description:"Normalizes features across batch dimension for 1D/2D input. Reduces internal covariate shift and enables higher learning rates. Essential for training stability in deep networks.",defaults:{num_features:128,eps:1e-5,momentum:.1,affine:!0,track_running_stats:!0},params:[{name:"num_features",type:"int",min:1,required:!0},{name:"eps",type:"float",min:0},{name:"momentum",type:"float",min:0,max:1},{name:"affine",type:"bool"},{name:"track_running_stats",type:"bool"}]},{category:"Normalization",friendly:"BatchNorm2d",torchClass:"torch.nn.BatchNorm2d",description:"Batch normalization for 2D spatial data (images). Normalizes across batch and spatial dimensions. Critical for stable training of deep convolutional networks.",defaults:{num_features:64,eps:1e-5,momentum:.1,affine:!0,track_running_stats:!0},params:[{name:"num_features",type:"int",min:1,required:!0},{name:"eps",type:"float",min:0},{name:"momentum",type:"float",min:0,max:1},{name:"affine",type:"bool"},{name:"track_running_stats",type:"bool"}]},{category:"Normalization",friendly:"LayerNorm",torchClass:"torch.nn.LayerNorm",description:"Normalizes features across feature dimension. Independent of batch size, making it suitable for RNNs and transformers. Computes statistics across the feature dimension.",defaults:{normalized_shape:128,eps:1e-5,elementwise_affine:!0},params:[{name:"normalized_shape",type:"int",min:1,required:!0},{name:"eps",type:"float",min:0},{name:"elementwise_affine",type:"bool"}]},{category:"Normalization",friendly:"GroupNorm",torchClass:"torch.nn.GroupNorm",description:"Group normalization divides channels into groups and normalizes within each group. Alternative to batch norm when batch sizes are small or variable.",defaults:{num_groups:2,num_channels:64,eps:1e-5,affine:!0},params:[{name:"num_groups",type:"int",min:1,required:!0},{name:"num_channels",type:"int",min:1,required:!0},{name:"eps",type:"float",min:0},{name:"affine",type:"bool"}]},{category:"Normalization",friendly:"InstanceNorm2d",torchClass:"torch.nn.InstanceNorm2d",description:"Instance normalization for style transfer and domain adaptation. Normalizes across spatial dimensions per instance. Helps remove instance-specific contrast information.",defaults:{num_features:64,eps:1e-5,momentum:.1,affine:!1,track_running_stats:!1},params:[{name:"num_features",type:"int",min:1,required:!0},{name:"eps",type:"float",min:0},{name:"momentum",type:"float",min:0,max:1},{name:"affine",type:"bool"},{name:"track_running_stats",type:"bool"}]},{category:"Loss",friendly:"CrossEntropyLoss",torchClass:"torch.nn.CrossEntropyLoss",description:"Cross-entropy loss for multi-class classification. Combines LogSoftmax and NLLLoss. Use for classification tasks with mutually exclusive classes. Supports class weighting and label smoothing.",defaults:{weight:null,ignore_index:-100,reduction:"mean",label_smoothing:0},params:[{name:"ignore_index",type:"int"},{name:"reduction",type:"select",options:["none","mean","sum"]},{name:"label_smoothing",type:"float",min:0,max:1}]},{category:"Loss",friendly:"MSELoss",torchClass:"torch.nn.MSELoss",description:"Mean Squared Error loss for regression tasks. Computes squared difference between predictions and targets. Sensitive to outliers, provides smooth gradients.",defaults:{reduction:"mean"},params:[{name:"reduction",type:"select",options:["none","mean","sum"]}]},{category:"Loss",friendly:"L1Loss",torchClass:"torch.nn.L1Loss",description:"L1 loss (Mean Absolute Error) for regression. Computes absolute difference between predictions and targets. More robust to outliers than MSE.",defaults:{reduction:"mean"},params:[{name:"reduction",type:"select",options:["none","mean","sum"]}]},{category:"Loss",friendly:"BCEWithLogitsLoss",torchClass:"torch.nn.BCEWithLogitsLoss",description:"Binary cross-entropy loss with logits for binary classification. Combines sigmoid and BCE loss for numerical stability. Use for binary classification tasks.",defaults:{weight:null,reduction:"mean",pos_weight:null},params:[{name:"reduction",type:"select",options:["none","mean","sum"]}]},{category:"Utility",friendly:"Add",torchClass:"torch.add",description:"Element-wise addition of two tensors. Essential for residual connections and skip connections in modern architectures like ResNet.",defaults:{},params:[]},{category:"Utility",friendly:"Concatenate",torchClass:"torch.cat",description:"Concatenates tensors along a specified dimension. Useful for combining features from different branches in complex architectures.",defaults:{dim:1},params:[{name:"dim",type:"int",required:!0}]},{category:"Utility",friendly:"ResNet Basic Block",torchClass:"blocks.ResNetBasicBlock",description:"ResNet basic block with two 3x3 convolutions, batch norm, and skip connection. Building block for ResNet-18 and ResNet-34 architectures.",defaults:{in_channels:64,out_channels:64,stride:1,downsample:!1},params:[{name:"in_channels",type:"int",min:1,required:!0},{name:"out_channels",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"downsample",type:"bool"}]},{category:"Utility",friendly:"ResNet Bottleneck Block",torchClass:"blocks.ResNetBottleneckBlock",description:"ResNet bottleneck block with 1x1, 3x3, 1x1 convolutions. Building block for ResNet-50, ResNet-101, and ResNet-152 architectures.",defaults:{in_channels:256,out_channels:64,stride:1,downsample:!1},params:[{name:"in_channels",type:"int",min:1,required:!0},{name:"out_channels",type:"int",min:1,required:!0},{name:"stride",type:"int",min:1},{name:"downsample",type:"bool"}]},{category:"Utility",friendly:"Inception Module",torchClass:"blocks.InceptionModule",description:"Inception module with parallel 1x1, 3x3, 5x5 convolutions and max pooling. Core building block for Inception/GoogLeNet architectures.",defaults:{in_channels:192,out_1x1:64,out_3x3_reduce:96,out_3x3:128,out_5x5_reduce:16,out_5x5:32,out_pool:32},params:[{name:"in_channels",type:"int",min:1,required:!0},{name:"out_1x1",type:"int",min:1,required:!0},{name:"out_3x3_reduce",type:"int",min:1,required:!0},{name:"out_3x3",type:"int",min:1,required:!0},{name:"out_5x5_reduce",type:"int",min:1,required:!0},{name:"out_5x5",type:"int",min:1,required:!0},{name:"out_pool",type:"int",min:1,required:!0}]},{category:"Utility",friendly:"Dropout",torchClass:"torch.nn.Dropout",description:"Dropout regularization randomly sets elements to zero during training. Prevents overfitting by reducing co-adaptation of neurons. Essential regularization technique.",defaults:{p:.5,inplace:!1},params:[{name:"p",type:"float",min:0,max:1},{name:"inplace",type:"bool"}]},{category:"Utility",friendly:"Identity",torchClass:"torch.nn.Identity",description:"Identity layer that returns input unchanged. Useful as placeholder in modular architectures or for debugging. No parameters or computation overhead.",defaults:{},params:[]}];var o=a(397);let s=[{category:"Training",friendly:"Training Config",torchClass:"training.config",description:"Core training configuration defining batch size, epochs, device settings. Connects to dataset input and trainable model output. Essential setup node for any training workflow.",defaults:{batch_size:32,epochs:10,device:"auto",num_workers:0,pin_memory:!0,shuffle:!0,drop_last:!1},params:[{name:"batch_size",type:"int",min:1,required:!0},{name:"epochs",type:"int",min:1,required:!0},{name:"device",type:"select",options:["auto","cpu","cuda","mps"]},{name:"num_workers",type:"int",min:0},{name:"pin_memory",type:"bool"},{name:"shuffle",type:"bool"},{name:"drop_last",type:"bool"}],inputType:"dataset",outputTypes:["trainable","prediction"],outputType:"trainable",inputShape:"[N, C, H, W]",outputShape:"[N, num_classes]"},{category:"DataAugmentation",friendly:"Random Rotation",torchClass:"torchvision.transforms.RandomRotation",description:"Randomly rotates input images by specified degrees. Improves model robustness to orientation changes. Essential for datasets where object rotation is common (e.g., natural images).",defaults:{degrees:30,interpolation:"nearest",expand:!1,center:null,fill:0},params:[{name:"degrees",type:"float",min:0,max:360,required:!0},{name:"interpolation",type:"select",options:["nearest","bilinear","bicubic"]},{name:"expand",type:"bool"},{name:"fill",type:"int",min:0,max:255}],inputType:"dataset",outputType:"dataset",inputShape:"[N, C, H, W]",outputShape:"[N, C, H, W]"},{category:"DataAugmentation",friendly:"Horizontal Flip",torchClass:"torchvision.transforms.RandomHorizontalFlip",description:"Randomly flips input images horizontally with given probability. Standard augmentation for natural images where horizontal symmetry is meaningful. Doubles effective dataset size.",defaults:{p:.5},params:[{name:"p",type:"float",min:0,max:1}],inputType:"dataset",outputType:"dataset",inputShape:"[N, C, H, W]",outputShape:"[N, C, H, W]"},{category:"DataAugmentation",friendly:"Vertical Flip",torchClass:"torchvision.transforms.RandomVerticalFlip",description:"Randomly flips input images vertically with given probability. Use when vertical symmetry is meaningful (e.g., aerial images, microscopy). Less common than horizontal flip.",defaults:{p:.5},params:[{name:"p",type:"float",min:0,max:1}],inputType:"dataset",outputType:"dataset",inputShape:"[N, C, H, W]",outputShape:"[N, C, H, W]"},{category:"DataAugmentation",friendly:"Random Resize",torchClass:"torchvision.transforms.RandomResizedCrop",description:"Randomly crops and resizes input to target size. Creates scale and aspect ratio invariance. Essential for training robust models on datasets with varying object sizes.",defaults:{size:224,scale_min:.08,scale_max:1,ratio_min:.75,ratio_max:1.33,interpolation:"bilinear"},params:[{name:"size",type:"int",min:1,required:!0},{name:"scale_min",type:"float",min:0,max:1},{name:"scale_max",type:"float",min:0,max:1},{name:"ratio_min",type:"float",min:0},{name:"ratio_max",type:"float",min:0},{name:"interpolation",type:"select",options:["nearest","bilinear","bicubic"]}],inputType:"dataset",outputType:"dataset",inputShape:"[N, C, H, W]",outputShape:"[N, C, H, W]"},{category:"DataAugmentation",friendly:"Random Crop",torchClass:"transforms.RandomCrop",description:"Randomly crops input image to given size with optional padding. Creates spatial invariance and data augmentation. Use padding to maintain image size after cropping.",defaults:{size:32,padding:0,pad_if_needed:!1,fill:0,padding_mode:"constant"},params:[{name:"size",type:"int",min:1,required:!0},{name:"padding",type:"int",min:0},{name:"pad_if_needed",type:"bool"},{name:"fill",type:"int",min:0,max:255},{name:"padding_mode",type:"select",options:["constant","edge","reflect","symmetric"]}],inputType:"dataset",outputType:"dataset",inputShape:"[N, C, H, W]",outputShape:"[N, C, H, W]"},{category:"DataAugmentation",friendly:"Color Jitter",torchClass:"torchvision.transforms.ColorJitter",description:"Randomly changes brightness, contrast, saturation, and hue. Improves model robustness to lighting conditions and color variations. Critical for real-world deployment.",defaults:{brightness:0,contrast:0,saturation:0,hue:0},params:[{name:"brightness",type:"float",min:0},{name:"contrast",type:"float",min:0},{name:"saturation",type:"float",min:0},{name:"hue",type:"float",min:-.5,max:.5}],inputType:"dataset",outputType:"dataset",inputShape:"[N, C, H, W]",outputShape:"[N, C, H, W]"},{category:"DataAugmentation",friendly:"Normalize",torchClass:"torchvision.transforms.Normalize",description:"Normalizes tensor with given mean and standard deviation. Essential preprocessing step. Use ImageNet statistics for transfer learning: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225].",defaults:{mean:[.485,.456,.406],std:[.229,.224,.225],inplace:!1},params:[{name:"inplace",type:"bool"}],inputType:"dataset",outputType:"dataset",inputShape:"[N, C, H, W]",outputShape:"[N, C, H, W]"},{category:"Optimization",friendly:"Adam",torchClass:"torch.optim.Adam",description:"Adam optimizer with adaptive learning rates. Computes individual learning rates for different parameters from estimates of first and second moments of gradients. Best general-purpose optimizer for most tasks.",defaults:{lr:.001,betas:[.9,.999],eps:1e-8,weight_decay:0,amsgrad:!1},params:[{name:"lr",type:"float",min:0,required:!0},{name:"weight_decay",type:"float",min:0},{name:"eps",type:"float",min:0},{name:"amsgrad",type:"bool"}],inputType:"trainable",outputType:"optimized",inputShape:"model_params",outputShape:"model_params"},{category:"Optimization",friendly:"SGD",torchClass:"torch.optim.SGD",description:"Stochastic Gradient Descent optimizer with optional momentum and weight decay. Classic optimization algorithm. Momentum helps accelerate convergence and reduces oscillations.",defaults:{lr:.01,momentum:0,dampening:0,weight_decay:0,nesterov:!1},params:[{name:"lr",type:"float",min:0,required:!0},{name:"momentum",type:"float",min:0},{name:"dampening",type:"float",min:0},{name:"weight_decay",type:"float",min:0},{name:"nesterov",type:"bool"}],inputType:"trainable",outputType:"optimized",inputShape:"model_params",outputShape:"model_params"},{category:"Optimization",friendly:"RMSprop",torchClass:"torch.optim.RMSprop",description:"RMSprop optimizer adapts learning rate based on recent gradient magnitudes. Good for non-stationary objectives and RNNs. Alpha controls moving average of squared gradients.",defaults:{lr:.01,alpha:.99,eps:1e-8,weight_decay:0,momentum:0,centered:!1},params:[{name:"lr",type:"float",min:0,required:!0},{name:"alpha",type:"float",min:0,max:1},{name:"eps",type:"float",min:0},{name:"weight_decay",type:"float",min:0},{name:"momentum",type:"float",min:0},{name:"centered",type:"bool"}],inputType:"trainable",outputType:"optimized",inputShape:"model_params",outputShape:"model_params"},{category:"Metrics",friendly:"Accuracy",torchClass:"metrics.Accuracy",description:"Classification accuracy metric measuring fraction of correct predictions. Most interpretable metric for balanced datasets. Use alongside other metrics for comprehensive evaluation.",defaults:{task:"multiclass",num_classes:10,top_k:1},params:[{name:"task",type:"select",options:["binary","multiclass","multilabel"]},{name:"num_classes",type:"int",min:2},{name:"top_k",type:"int",min:1}],inputType:"prediction",outputType:"metric",inputShape:"[N, num_classes]",outputShape:"scalar"},{category:"Metrics",friendly:"Precision",torchClass:"metrics.precision",description:"Precision metric: TP/(TP+FP). Measures fraction of positive predictions that are correct. Important when false positives are costly. Use with recall for comprehensive evaluation.",defaults:{task:"multiclass",num_classes:10,average:"macro"},params:[{name:"task",type:"select",options:["binary","multiclass","multilabel"]},{name:"num_classes",type:"int",min:2},{name:"average",type:"select",options:["micro","macro","weighted","none"]}],inputType:"prediction",outputType:"metric",inputShape:"[N, num_classes]",outputShape:"scalar"},{category:"Metrics",friendly:"Recall",torchClass:"metrics.recall",description:"Recall metric: TP/(TP+FN). Measures fraction of actual positives correctly identified. Critical when false negatives are costly (e.g., medical diagnosis). Complements precision.",defaults:{task:"multiclass",num_classes:10,average:"macro"},params:[{name:"task",type:"select",options:["binary","multiclass","multilabel"]},{name:"num_classes",type:"int",min:2},{name:"average",type:"select",options:["micro","macro","weighted","none"]}],inputType:"prediction",outputType:"metric",inputShape:"[N, num_classes]",outputShape:"scalar"},{category:"Metrics",friendly:"F1 Score",torchClass:"metrics.f1_score",description:"F1 Score: harmonic mean of precision and recall (2*P*R/(P+R)). Balances precision and recall. Excellent single metric for imbalanced datasets and binary classification.",defaults:{task:"multiclass",num_classes:10,average:"macro"},params:[{name:"task",type:"select",options:["binary","multiclass","multilabel"]},{name:"num_classes",type:"int",min:2},{name:"average",type:"select",options:["micro","macro","weighted","none"]}],inputType:"prediction",outputType:"metric",inputShape:"[N, num_classes]",outputShape:"scalar"},{category:"Callbacks",friendly:"Model Checkpoint",torchClass:"callbacks.ModelCheckpoint",description:"Saves model checkpoints during training based on monitored metric. Prevents loss of progress and enables model recovery. Essential for long training runs and model selection.",defaults:{monitor:"val_loss",mode:"min",save_top_k:1,save_last:!1,verbose:!1},params:[{name:"monitor",type:"select",options:["val_loss","val_accuracy","train_loss","train_accuracy"]},{name:"mode",type:"select",options:["min","max"]},{name:"save_top_k",type:"int",min:-1},{name:"save_last",type:"bool"},{name:"verbose",type:"bool"}],inputType:"metric",outputType:"saved_model",inputShape:"scalar",outputShape:"saved_state"},{category:"Callbacks",friendly:"Early Stopping",torchClass:"callbacks.EarlyStopping",description:"Stops training when monitored metric stops improving. Prevents overfitting and saves computational resources. Connect after loss/metric nodes for automatic training termination.",defaults:{monitor:"val_loss",patience:7,mode:"min",min_delta:0,verbose:!1,restore_best_weights:!1},params:[{name:"monitor",type:"select",options:["val_loss","val_accuracy","train_loss","train_accuracy"]},{name:"patience",type:"int",min:1},{name:"mode",type:"select",options:["min","max"]},{name:"min_delta",type:"float",min:0},{name:"verbose",type:"bool"},{name:"restore_best_weights",type:"bool"}],inputType:"metric",outputType:"stop_signal",inputShape:"scalar",outputShape:"boolean"},{category:"Callbacks",friendly:"Learning Rate Monitor",torchClass:"callbacks.LearningRateMonitor",description:"Monitors and logs learning rate changes during training. Essential for understanding optimizer behavior and diagnosing training issues. Connects to optimizer output.",defaults:{logging_interval:"epoch",log_momentum:!1},params:[{name:"logging_interval",type:"select",options:["step","epoch"]},{name:"log_momentum",type:"bool"}],inputType:"optimized",outputType:"log",inputShape:"model_params",outputShape:"log_entry"}],l={"torchvision.datasets.MNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.FashionMNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.CIFAR10":{shape:[3,32,32],type:"float32"},"torchvision.datasets.CIFAR100":{shape:[3,32,32],type:"float32"},"torchvision.datasets.ImageNet":{shape:[3,224,224],type:"float32"},"torchvision.datasets.SVHN":{shape:[3,32,32],type:"float32"},"torchvision.datasets.EMNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.KMNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.QMNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.STL10":{shape:[3,96,96],type:"float32"},"torchvision.datasets.CelebA":{shape:[3,218,178],type:"float32"}};Object.keys(l).map(e=>({label:e.split(".").pop(),value:e}));let d=(e,t,a,n,i,r)=>{let o={version:"1.0.0",timestamp:Date.now(),modelNodes:e,modelEdges:t,trainingNodes:a,trainingEdges:n,code:i,mode:r};try{let e=JSON.stringify(o,null,2),t=new Blob([e],{type:"application/json"}),a=URL.createObjectURL(t),n=document.createElement("a");n.href=a,n.download="nn-builder-project-".concat(new Date().toISOString().slice(0,10),".json"),document.body.appendChild(n),n.click(),document.body.removeChild(n),URL.revokeObjectURL(a),localStorage.setItem("nn-builder-last-save",e),console.log("Project saved successfully")}catch(e){console.error("Failed to save project:",e),alert("Failed to save project. Please try again.")}},c=()=>new Promise(e=>{let t=document.createElement("input");t.type="file",t.accept=".json",t.onchange=t=>{var a;let n=null==(a=t.target.files)?void 0:a[0];if(!n)return void e(null);let i=new FileReader;i.onload=t=>{try{var a;let n=null==(a=t.target)?void 0:a.result,i=JSON.parse(n);if(!i.version||!i.modelNodes||!i.trainingNodes)throw Error("Invalid file format");console.log("Project loaded successfully"),e(i)}catch(t){console.error("Failed to load project:",t),alert("Failed to load project. Please check the file format."),e(null)}},i.readAsText(n)},t.oncancel=()=>{e(null)},t.click()}),u=()=>({version:"1.0.0",timestamp:Date.now(),modelNodes:[],modelEdges:[],trainingNodes:[],trainingEdges:[],code:'# Click "Generate Code" to generate PyTorch code from your model',mode:"model"}),p=e=>{if("input.dataset"===e.data.registryKey){var t,a;let n=(null==(a=l[(null==(t=e.data.params)?void 0:t.dataset)||"torchvision.datasets.MNIST"])?void 0:a.shape)||[1,28,28];return{outputType:"dataset",outputShape:"[N, ".concat(n.join(", "),"]"),inputType:void 0,inputShape:void 0,category:"DataAugmentation"}}if(e.data.isTraining){let t=s.find(t=>t.torchClass===e.data.registryKey);if(t)return"training.config"===e.data.registryKey?{...t,outputType:"prediction",outputTypes:["trainable","prediction"],inputType:t.inputType||void 0,inputShape:t.inputShape||void 0,outputShape:t.outputShape||void 0,category:t.category}:{...t,outputType:t.outputType||void 0,outputTypes:t.outputTypes,inputType:t.inputType||void 0,inputShape:t.inputShape||void 0,outputShape:t.outputShape||void 0,category:t.category}}return r.find(t=>t.torchClass===e.data.registryKey)},m=(e,t)=>t?{...e,data:{...e.data,outputShape:t.outputShape,outputType:t.outputType,outputTypes:t.outputTypes}}:e,h=(e,t)=>{if(!t.source||!t.target)return console.log("DEBUG: Missing source or target connection"),!1;let a=e.find(e=>e.id===t.source),n=e.find(e=>e.id===t.target);if(!a||!n)return console.log("DEBUG: Could not find source or target node",{sourceId:t.source,targetId:t.target}),!1;if(!a.data.isTraining||!n.data.isTraining)return console.log("DEBUG: Both nodes must be training nodes"),!1;let i=p(a),r=p(n);if(!i||!r)return console.log("DEBUG: Could not find metadata for nodes",{source:a.data.registryKey,target:n.data.registryKey}),!1;"input.dataset"===a.data.registryKey?a.data.shape:i.outputShape;let o="training.config"===a.data.registryKey,s="Metrics"===r.category,l="Optimization"===r.category,d=r.inputType,c=i.outputType,u=i.outputTypes||[];o&&(s?c="prediction":l&&(c="trainable"));let m=c===d||u.includes(d||"");return(console.log("[DEBUG isValidTrainingConnection]",{sourceNode:a.data.registryKey,targetNode:n.data.registryKey,sourceMeta:i,targetMeta:r,sourceOutputType:c,sourceOutputTypes:u,targetInputType:d,isTrainingConfig:o,isMetricTarget:s,isOptimizerTarget:l,isValidType:m}),m)?!!("input.dataset"===a.data.registryKey||"DataAugmentation"===i.category?"DataAugmentation"===r.category||"Training"===r.category:"Training"===i.category?"Optimization"===r.category||"Metrics"===r.category||"Callbacks"===r.category:"Optimization"===i.category?"Training"===r.category||"Callbacks"===r.category:"Metrics"===i.category?"Callbacks"===r.category:"Callbacks"!==i.category&&!1)||(console.log("DEBUG: Invalid workflow",{sourceCategory:i.category,targetCategory:r.category,sourceNode:a.data.registryKey,targetNode:n.data.registryKey}),!1):(console.log("DEBUG: Type mismatch",{sourceNode:a.data.registryKey,targetNode:n.data.registryKey,sourceOutput:c,sourceOutputs:u,targetInput:d,sourceCategory:i.category,targetCategory:r.category,isTrainingConfig:o,isMetricTarget:s,isOptimizerTarget:l,sourceMeta:i,targetMeta:r}),!1)},f=(e,t)=>{if(!t.source||!t.target)return!1;let a=e.find(e=>e.id===t.source),n=e.find(e=>e.id===t.target);return!!a&&!!n&&!a.data.isTraining&&!n.data.isTraining&&"input.dataset"!==n.data.registryKey},g=e=>({id:"training-dataset",type:"default",position:{x:100,y:100},data:{registryKey:"input.dataset",label:"Training Dataset",params:e?{...e.data.params}:{dataset:"torchvision.datasets.MNIST"},isTraining:!0,shape:null==e?void 0:e.data.shape,dtype:null==e?void 0:e.data.dtype}}),y=(0,o.v)((e,t)=>({nodes:[],edges:[],mode:"model",code:'# Click "Generate Code" to generate PyTorch code from your model',modelNodes:[],trainingNodes:[],modelEdges:[],trainingEdges:[],setNodes:t=>e(e=>{let a="function"==typeof t?t(e.nodes):t,n=a.map(t=>{let n=e.edges.filter(e=>e.target===t.id);if(0===n.length)return t;let i=a.find(e=>e.id===n[0].source);return i?m(t,p(i)):t});return{nodes:n,..."model"===e.mode?{modelNodes:n}:{trainingNodes:n}}}),setEdges:t=>e(e=>{let a="function"==typeof t?t(e.edges):t,n=e.nodes.map(t=>{let n=a.filter(e=>e.target===t.id);if(0===n.length)return t;let i=e.nodes.find(e=>e.id===n[0].source);return i?m(t,p(i)):t});return{edges:a,nodes:n,..."model"===e.mode?{modelEdges:a,modelNodes:n}:{trainingEdges:a,trainingNodes:n}}}),setMode:t=>e(e=>{if("training"===t&&0===e.trainingNodes.length){let a=g(e.modelNodes.find(e=>"input.dataset"===e.data.registryKey));return{mode:t,trainingNodes:[a],nodes:[a],edges:[]}}return"code"===t?{mode:t}:{mode:t,nodes:"model"===t?e.modelNodes:e.trainingNodes,edges:"model"===t?e.modelEdges||[]:e.trainingEdges||[]}}),setModelNodes:t=>e(e=>({modelNodes:"function"==typeof t?t(e.modelNodes):t,..."model"===e.mode&&{nodes:"function"==typeof t?t(e.modelNodes):t}})),setTrainingNodes:t=>e(e=>({trainingNodes:"function"==typeof t?t(e.trainingNodes):t,..."training"===e.mode&&{nodes:"function"==typeof t?t(e.trainingNodes):t}})),setCode:t=>e(e=>({code:t})),isValidConnection:e=>{let a=t();return"training"===a.mode?h(a.nodes,e):f(a.nodes,e)},saveProject:()=>{let e=t();d(e.modelNodes,e.modelEdges,e.trainingNodes,e.trainingEdges,e.code,e.mode)},loadProject:async()=>{let t=await c();t&&e({modelNodes:t.modelNodes,modelEdges:t.modelEdges,trainingNodes:t.trainingNodes,trainingEdges:t.trainingEdges,code:t.code,mode:t.mode,nodes:"model"===t.mode?t.modelNodes:t.trainingNodes,edges:"model"===t.mode?t.modelEdges:t.trainingEdges})},clearProject:()=>{let t=u();e({modelNodes:t.modelNodes,modelEdges:t.modelEdges,trainingNodes:t.trainingNodes,trainingEdges:t.trainingEdges,code:t.code,mode:t.mode,nodes:t.modelNodes,edges:t.modelEdges})}}));var b=a(5520);let v={Input:{header:"bg-yellow-100 text-yellow-800",item:"bg-yellow-500 hover:bg-yellow-600",border:"border-yellow-300"},Layers:{header:"bg-blue-100 text-blue-800",item:"bg-blue-500 hover:bg-blue-600",border:"border-blue-300"},Activations:{header:"bg-green-100 text-green-800",item:"bg-green-500 hover:bg-green-600",border:"border-green-300"},Pooling:{header:"bg-orange-100 text-orange-800",item:"bg-orange-500 hover:bg-orange-600",border:"border-orange-300"},Normalization:{header:"bg-purple-100 text-purple-800",item:"bg-purple-500 hover:bg-purple-600",border:"border-purple-300"},Loss:{header:"bg-red-100 text-red-800",item:"bg-red-500 hover:bg-red-600",border:"border-red-300"},Utility:{header:"bg-gray-100 text-gray-800",item:"bg-gray-500 hover:bg-gray-600",border:"border-gray-300"}};function x(e){let{layer:t,position:a,onClose:i}=e;if(!t||!a)return null;let r={"torch.nn.Linear":"Mathematical Formula: y = xW^T + b\nWhere W is weight matrix, b is bias vector","torch.nn.Conv2d":"Mathematical Formula: (f * g)(t) = ∫ f(τ)g(t-τ)dτ\nConvolution operation with learnable kernels","torch.nn.ReLU":"Mathematical Formula: ReLU(x) = max(0, x)\nPiecewise linear function","torch.nn.LeakyReLU":"Mathematical Formula: LeakyReLU(x) = max(αx, x)\nWhere α is negative slope (typically 0.01)","torch.nn.Sigmoid":"Mathematical Formula: σ(x) = 1/(1 + e^(-x))\nSquashes values to (0,1) range","torch.nn.Tanh":"Mathematical Formula: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))\nSquashes values to (-1,1) range","torch.nn.BatchNorm2d":"Mathematical Formula: y = (x - μ) / √(σ\xb2 + ε) * γ + β\nNormalizes across batch dimension","torch.nn.LSTM":"Uses forget, input, and output gates with cell state\nLong-term memory through gating mechanisms","torch.nn.MaxPool2d":"Takes maximum value in each pooling window\nReduces spatial dimensions while preserving features"}[t.torchClass]||null,o={"torch.nn.Linear":["Classification heads","Feature transformation","Dense connections","Output layers"],"torch.nn.Conv2d":["Image feature extraction","Pattern recognition","Spatial hierarchy learning","CNNs"],"torch.nn.ReLU":["Hidden layer activation","Solving vanishing gradients","Computational efficiency"],"torch.nn.LSTM":["Sequence modeling","NLP tasks","Time series prediction","Long-term dependencies"],"torch.nn.BatchNorm2d":["Accelerating training","Reducing internal covariate shift","Regularization"],"torch.nn.Dropout":["Preventing overfitting","Model regularization","Improving generalization"]}[t.torchClass]||["General purpose neural network component"];return(0,n.jsxs)("div",{className:"fixed z-[10000] w-96 bg-gray-900 text-white text-sm rounded-lg shadow-xl border border-gray-700",style:{left:a.x+10,top:a.y,maxHeight:"400px",overflowY:"auto"},onMouseLeave:i,children:[(0,n.jsxs)("div",{className:"p-4 space-y-3",children:[(0,n.jsxs)("div",{className:"border-b border-gray-700 pb-2",children:[(0,n.jsx)("h3",{className:"text-lg font-bold text-blue-300",children:t.friendly}),(0,n.jsx)("p",{className:"text-xs text-gray-400 font-mono",children:t.torchClass})]}),(0,n.jsxs)("div",{children:[(0,n.jsx)("h4",{className:"font-semibold text-green-300 mb-1",children:"Description"}),(0,n.jsx)("p",{className:"text-gray-200 text-xs leading-relaxed",children:t.description})]}),r&&(0,n.jsxs)("div",{children:[(0,n.jsx)("h4",{className:"font-semibold text-purple-300 mb-1",children:"Mathematics"}),(0,n.jsx)("pre",{className:"text-gray-200 text-xs bg-gray-800 p-2 rounded whitespace-pre-wrap",children:r})]}),(0,n.jsxs)("div",{children:[(0,n.jsx)("h4",{className:"font-semibold text-yellow-300 mb-1",children:"Common Use Cases"}),(0,n.jsx)("ul",{className:"text-gray-200 text-xs space-y-1",children:o.map((e,t)=>(0,n.jsxs)("li",{className:"flex items-start",children:[(0,n.jsx)("span",{className:"text-yellow-400 mr-2",children:"•"}),(0,n.jsx)("span",{children:e})]},t))})]}),t.params.length>0&&(0,n.jsxs)("div",{children:[(0,n.jsx)("h4",{className:"font-semibold text-cyan-300 mb-1",children:"Key Parameters"}),(0,n.jsxs)("div",{className:"space-y-1",children:[t.params.slice(0,4).map(e=>(0,n.jsxs)("div",{className:"text-xs",children:[(0,n.jsx)("span",{className:"text-cyan-200 font-medium",children:e.name}),(0,n.jsxs)("span",{className:"text-gray-400",children:[" (",e.type,")"]}),e.required&&(0,n.jsx)("span",{className:"text-red-400",children:" *required"})]},e.name)),t.params.length>4&&(0,n.jsxs)("p",{className:"text-gray-500 text-xs",children:["... and ",t.params.length-4," more parameters"]})]})]}),(0,n.jsx)("div",{className:"border-t border-gray-700 pt-2",children:(0,n.jsx)("p",{className:"text-gray-500 text-xs",children:"Right-click to show • Drag to add to canvas"})})]}),(0,n.jsx)("div",{className:"absolute top-4 -left-2 w-4 h-4 bg-gray-900 border-l border-t border-gray-700 transform rotate-45"})]})}function _(){let{setNodes:e,setEdges:t,saveProject:a,loadProject:o,clearProject:s}=y(),l=(0,i.useCallback)(()=>{e([]),t([]);let a=[],n={id:(0,b.Ak)(6),type:"default",position:{x:100,y:100},data:{registryKey:"torch.nn.Conv2d",label:"input_conv",params:{in_channels:3,out_channels:64,kernel_size:7,stride:2,padding:3,bias:!1},isTraining:!1}};a.push(n);let i={id:(0,b.Ak)(6),type:"default",position:{x:300,y:100},data:{registryKey:"torch.nn.BatchNorm2d",label:"input_bn",params:{num_features:64},isTraining:!1}};a.push(i);let r={id:(0,b.Ak)(6),type:"default",position:{x:500,y:100},data:{registryKey:"torch.nn.ReLU",label:"input_relu",params:{inplace:!0},isTraining:!1}};a.push(r);let o={id:(0,b.Ak)(6),type:"default",position:{x:700,y:100},data:{registryKey:"torch.nn.MaxPool2d",label:"input_pool",params:{kernel_size:3,stride:2,padding:1},isTraining:!1}};a.push(o),[{x:100,y:200},{x:300,y:200},{x:500,y:200}].forEach((e,t)=>{let n={id:(0,b.Ak)(6),type:"default",position:e,data:{registryKey:"blocks.ResNetBasicBlock",label:"resblock_".concat(t+1),params:{in_channels:64,out_channels:64,stride:1,downsample:!1},isTraining:!1}};a.push(n)});let s={id:(0,b.Ak)(6),type:"default",position:{x:700,y:200},data:{registryKey:"torch.nn.AdaptiveAvgPool2d",label:"global_pool",params:{output_size:1},isTraining:!1}};a.push(s);let l={id:(0,b.Ak)(6),type:"default",position:{x:900,y:200},data:{registryKey:"torch.nn.Flatten",label:"flatten",params:{start_dim:1},isTraining:!1}};a.push(l);let d={id:(0,b.Ak)(6),type:"default",position:{x:1100,y:200},data:{registryKey:"torch.nn.Linear",label:"classifier",params:{in_features:64,out_features:10},isTraining:!1}};a.push(d);let c=[{id:"".concat(n.id,"-").concat(i.id),source:n.id,target:i.id,type:"default"},{id:"".concat(i.id,"-").concat(r.id),source:i.id,target:r.id,type:"default"},{id:"".concat(r.id,"-").concat(o.id),source:r.id,target:o.id,type:"default"},{id:"".concat(o.id,"-").concat(a[4].id),source:o.id,target:a[4].id,type:"default"},{id:"".concat(a[4].id,"-").concat(a[5].id),source:a[4].id,target:a[5].id,type:"default"},{id:"".concat(a[5].id,"-").concat(a[6].id),source:a[5].id,target:a[6].id,type:"default"},{id:"".concat(a[6].id,"-").concat(s.id),source:a[6].id,target:s.id,type:"default"},{id:"".concat(s.id,"-").concat(l.id),source:s.id,target:l.id,type:"default"},{id:"".concat(l.id,"-").concat(d.id),source:l.id,target:d.id,type:"default"}];e(e=>[...e,...a]),t(e=>[...e,...c])},[e,t]),d=(0,i.useCallback)(()=>{e([]),t([]);let a=[],n={id:(0,b.Ak)(6),type:"default",position:{x:100,y:100},data:{registryKey:"torch.nn.Conv2d",label:"conv1",params:{in_channels:3,out_channels:64,kernel_size:7,stride:2,padding:3,bias:!1},isTraining:!1}};a.push(n);let i={id:(0,b.Ak)(6),type:"default",position:{x:350,y:100},data:{registryKey:"torch.nn.MaxPool2d",label:"maxpool1",params:{kernel_size:3,stride:2,padding:1},isTraining:!1}};a.push(i);let r={id:(0,b.Ak)(6),type:"default",position:{x:100,y:220},data:{registryKey:"blocks.InceptionModule",label:"inception1",params:{in_channels:64,out_1x1:32,out_3x3_reduce:48,out_3x3:64,out_5x5_reduce:8,out_5x5:16,out_pool:16},isTraining:!1}};a.push(r);let o={id:(0,b.Ak)(6),type:"default",position:{x:350,y:220},data:{registryKey:"blocks.InceptionModule",label:"inception2",params:{in_channels:128,out_1x1:64,out_3x3_reduce:96,out_3x3:128,out_5x5_reduce:16,out_5x5:32,out_pool:32},isTraining:!1}};a.push(o);let s={id:(0,b.Ak)(6),type:"default",position:{x:600,y:220},data:{registryKey:"torch.nn.AdaptiveAvgPool2d",label:"global_pool",params:{output_size:1},isTraining:!1}};a.push(s);let l={id:(0,b.Ak)(6),type:"default",position:{x:850,y:220},data:{registryKey:"torch.nn.Flatten",label:"flatten",params:{start_dim:1},isTraining:!1}};a.push(l);let d={id:(0,b.Ak)(6),type:"default",position:{x:1100,y:220},data:{registryKey:"torch.nn.Linear",label:"classifier",params:{in_features:256,out_features:10},isTraining:!1}};a.push(d);let c=[{id:"".concat(n.id,"-").concat(i.id),source:n.id,target:i.id,type:"default"},{id:"".concat(i.id,"-").concat(r.id),source:i.id,target:r.id,type:"default"},{id:"".concat(r.id,"-").concat(o.id),source:r.id,target:o.id,type:"default"},{id:"".concat(o.id,"-").concat(s.id),source:o.id,target:s.id,type:"default"},{id:"".concat(s.id,"-").concat(l.id),source:s.id,target:l.id,type:"default"},{id:"".concat(l.id,"-").concat(d.id),source:l.id,target:d.id,type:"default"}];e(e=>[...e,...a]),t(e=>[...e,...c])},[e,t]),[c,u]=(0,i.useState)(()=>{let e={};return["Input",...new Set(r.map(e=>e.category).filter(e=>"Input"!==e))].forEach(t=>{e[t]=!0}),e}),[p,m]=(0,i.useState)({layer:null,position:null}),h=(e,t)=>{e.preventDefault(),m({layer:t,position:{x:e.clientX,y:e.clientY}})},f=async()=>{await o()};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)("aside",{className:"w-56 overflow-y-auto border-r bg-white",style:{height:"calc(100vh - 48px)"},children:[(0,n.jsxs)("div",{className:"sticky top-0 z-10 bg-white border-b mb-2 flex items-center gap-2 px-2 py-2",children:[(0,n.jsxs)("div",{className:"relative group",children:[(0,n.jsx)("button",{className:"px-3 py-1 rounded hover:bg-gray-100 font-semibold text-gray-900",children:"File"}),(0,n.jsx)("div",{className:"absolute left-0 top-[calc(100%-2px)] w-48 bg-white border rounded-md shadow-lg hidden group-hover:block",children:(0,n.jsxs)("div",{className:"py-1",children:[(0,n.jsx)("button",{onClick:()=>{confirm("Clear the current network? This cannot be undone.")&&s()},className:"w-full text-left px-4 py-2 hover:bg-gray-100 text-gray-900",children:"New"}),(0,n.jsx)("button",{onClick:()=>{a()},className:"w-full text-left px-4 py-2 hover:bg-gray-100 text-gray-900",children:"Save"}),(0,n.jsx)("button",{onClick:f,className:"w-full text-left px-4 py-2 hover:bg-gray-100 text-gray-900",children:"Load"}),(0,n.jsx)("div",{className:"border-t border-gray-200 my-1"}),(0,n.jsx)("div",{className:"px-4 py-1 text-xs font-semibold text-gray-500 uppercase tracking-wide",children:"Load Templates"}),(0,n.jsx)("button",{onClick:l,className:"w-full text-left px-4 py-2 hover:bg-gray-100 text-gray-900",children:"ResNet Model"}),(0,n.jsx)("button",{onClick:d,className:"w-full text-left px-4 py-2 hover:bg-gray-100 text-gray-900",children:"Inception Model"})]})})]}),(0,n.jsxs)("div",{className:"relative group",children:[(0,n.jsx)("button",{className:"px-3 py-1 rounded hover:bg-gray-100 font-semibold text-gray-900",children:"View"}),(0,n.jsx)("div",{className:"absolute left-0 top-[calc(100%-2px)] w-48 bg-white border rounded-md shadow-lg hidden group-hover:block",children:(0,n.jsxs)("div",{className:"py-1",children:[(0,n.jsx)("button",{onClick:()=>{u(e=>{let t={};return Object.keys(e).forEach(e=>{t[e]=!0}),t})},className:"w-full text-left px-4 py-2 hover:bg-gray-100 text-gray-900",children:"Expand All"}),(0,n.jsx)("button",{onClick:()=>{u(e=>{let t={};return Object.keys(e).forEach(e=>{t[e]=!1}),t})},className:"w-full text-left px-4 py-2 hover:bg-gray-100 text-gray-900",children:"Collapse All"})]})})]})]}),(0,n.jsx)("div",{className:"p-3 space-y-4",children:["Input",...new Set(r.map(e=>e.category).filter(e=>"Input"!==e))].map(e=>{var t,a,o;return(0,n.jsxs)(i.Fragment,{children:[(0,n.jsxs)("button",{className:"w-full flex items-center justify-between px-2 py-1 rounded font-bold uppercase text-xs mb-1 transition-colors duration-150 ".concat((null==(t=v[e])?void 0:t.header)||""),onClick:()=>u(t=>({...t,[e]:!t[e]})),type:"button",children:[(0,n.jsx)("span",{children:e}),(0,n.jsx)("span",{className:"ml-2",children:c[e]?"▾":"▸"})]}),(0,n.jsx)("div",{className:"space-y-1 pl-2 border-l-4 ".concat((null==(a=v[e])?void 0:a.border)||""," transition-all duration-200 ").concat(c[e]?"max-h-[1000px] opacity-100":"max-h-0 opacity-0 overflow-hidden"),style:{transitionProperty:"max-height, opacity"},children:c[e]&&("Input"===e?(0,n.jsx)("div",{draggable:!0,onDragStart:e=>{e.dataTransfer.setData("application/reactflow",JSON.stringify({registryKey:"input.dataset"})),e.dataTransfer.effectAllowed="move"},onContextMenu:e=>{e.preventDefault(),h(e,{category:"Input",friendly:"Dataset",torchClass:"input.dataset",description:"Input dataset node providing training and validation data to the neural network. Represents the entry point for your machine learning pipeline. Supports popular computer vision datasets like MNIST, CIFAR-10, ImageNet, and more.",defaults:{},params:[]})},className:"cursor-grab rounded px-2 py-1 text-sm shadow transition-colors duration-100 ".concat((null==(o=v[e])?void 0:o.item)||""),children:"Dataset"},"Dataset"):r.filter(t=>t.category===e).map(t=>{var a;return(0,n.jsx)("div",{draggable:!0,onDragStart:e=>{e.dataTransfer.setData("application/reactflow",JSON.stringify({registryKey:t.torchClass})),e.dataTransfer.effectAllowed="move"},onContextMenu:e=>h(e,t),className:"cursor-grab rounded px-2 py-1 text-sm shadow transition-colors duration-100 ".concat((null==(a=v[e])?void 0:a.item)||""),children:t.friendly},t.friendly)}))})]},e)})})]}),(0,n.jsx)(x,{layer:p.layer,position:p.position,onClose:()=>{m({layer:null,position:null})}})]})}var k=a(6254),w=a(4572),N=a(3979),C=a(4740);a(2204);var T=a(8819);let j={in_features:{description:"Number of input features to the linear layer",advice:"Should match the output size of the previous layer. For flattened images: height \xd7 width \xd7 channels",defaultValue:128},out_features:{description:"Number of output features from the linear layer",advice:"Set to desired output size. For classification, use number of classes. For hidden layers, typically 64-512",defaultValue:64},bias:{description:"Whether to use bias terms in the layer",advice:"Keep True unless you have batch normalization immediately after. Bias helps model fit better",defaultValue:!0},in_channels:{description:"Number of input channels (depth) to the convolution",advice:"Must match previous layer output channels. RGB images = 3, grayscale = 1",defaultValue:3},out_channels:{description:"Number of output channels (filters) from the convolution",advice:"More channels = more features learned. Start with 32-64, increase in deeper layers (64→128→256)",defaultValue:32},kernel_size:{description:"Size of the convolution kernel (filter)",advice:"3\xd73 or 5\xd75 are common. Larger kernels capture more spatial context but increase computation",defaultValue:3},stride:{description:"Step size when moving the kernel across the input",advice:"1 = no downsampling. 2 = halves spatial dimensions. Use >1 for downsampling instead of pooling",defaultValue:1},padding:{description:"Zero-padding added to input borders",advice:"Use (kernel_size-1)/2 to maintain spatial dimensions. 0 = shrink output size",defaultValue:0},dilation:{description:"Spacing between kernel elements (dilated convolution)",advice:"1 = standard convolution. >1 = dilated/atrous convolution for larger receptive field",defaultValue:1},groups:{description:"Number of groups for grouped convolution",advice:"1 = standard convolution. >1 = grouped/depthwise convolution for efficiency",defaultValue:1},padding_mode:{description:"Type of padding to apply at borders",advice:"'zeros' for most cases. 'reflect'/'replicate' for images to avoid border artifacts",defaultValue:"zeros"},inplace:{description:"Whether to modify input tensor in-place to save memory",advice:"True saves memory but prevents gradient computation. Use False during training, True for inference",defaultValue:!1},negative_slope:{description:"Slope for negative values in LeakyReLU",advice:"Small values like 0.01 help prevent dying neurons. Larger values (0.1-0.2) for more leakage",defaultValue:.01},elu_alpha:{description:"Alpha parameter for ELU activation",advice:"Controls saturation for negative inputs. 1.0 is standard. Larger values = more negative saturation",defaultValue:1},approximate:{description:"Approximation method for GELU",advice:"'none' for exact computation. 'tanh' for faster approximation in inference",defaultValue:"none"},dim:{description:"Dimension along which to apply softmax",advice:"Usually last dimension (-1 or 1). For classification, use dimension with class scores",defaultValue:null},beta:{description:"Beta parameter for Softplus activation",advice:"Controls steepness. Larger values make it more like ReLU. 1 is standard",defaultValue:1},threshold:{description:"Threshold above which to use linear approximation",advice:"For numerical stability. 20 is standard, rarely needs changing",defaultValue:20},min_val:{description:"Minimum output value for Hardtanh",advice:"Lower bound for clamping. -1 is standard for tanh-like behavior",defaultValue:-1},max_val:{description:"Maximum output value for Hardtanh",advice:"Upper bound for clamping. 1 is standard for tanh-like behavior",defaultValue:1},return_indices:{description:"Whether to return indices of maximum values",advice:"True if you need unpooling later. False for standard pooling to save memory",defaultValue:!1},ceil_mode:{description:"Whether to use ceiling instead of floor for output size calculation",advice:"False for standard behavior. True to include partial windows at borders",defaultValue:!1},count_include_pad:{description:"Whether to include padding in average calculation",advice:"True includes zeros in average (standard). False excludes padding for more accurate averages",defaultValue:!0},divisor_override:{description:"Override the divisor used in average pooling",advice:"Leave null for automatic calculation. Set manually only for special normalization needs",defaultValue:null},output_size:{description:"Target output size for adaptive pooling",advice:"Common sizes: 1 for global pooling, 7 for classification heads, (H,W) for specific sizes",defaultValue:1},num_features:{description:"Number of features/channels to normalize",advice:"Must match input channels. Same as out_channels from previous conv layer",defaultValue:64},norm_eps:{description:"Small value added to denominator for numerical stability",advice:"1e-5 is standard. Increase if you get NaN errors, decrease for more precision",defaultValue:1e-5},norm_momentum:{description:"Momentum for running statistics update",advice:"0.1 is standard. Lower = smoother updates. Higher = more responsive to recent batches",defaultValue:.1},affine:{description:"Whether to use learnable scale and shift parameters",advice:"True for learnable normalization. False for just normalization without scaling",defaultValue:!0},track_running_stats:{description:"Whether to track running mean and variance",advice:"True for training/inference difference. False for always using batch statistics",defaultValue:!0},normalized_shape:{description:"Shape of the input to be normalized",advice:"Usually the feature dimension size. For transformers, use embedding dimension",defaultValue:128},elementwise_affine:{description:"Whether to use learnable per-element scale and shift",advice:"True for learnable normalization. False for standard normalization only",defaultValue:!0},num_groups:{description:"Number of groups to divide channels into",advice:"Lower = more normalization groups. Common: 8, 16, 32. Must divide num_channels evenly",defaultValue:2},num_channels:{description:"Total number of channels in the input",advice:"Must match input channels. Should be divisible by num_groups",defaultValue:64},input_size:{description:"Number of expected features in input",advice:"For embeddings: embedding_dim. For sequences: feature size per timestep",defaultValue:128},hidden_size:{description:"Number of features in hidden state",advice:"Larger = more memory/capacity. Common: 64-512 for small tasks, 512-2048 for large",defaultValue:64},num_layers:{description:"Number of recurrent layers stacked",advice:"1-3 layers common. More layers = more capacity but harder to train",defaultValue:1},batch_first:{description:"Whether input/output tensors have batch dimension first",advice:"False: (seq_len, batch, features). True: (batch, seq_len, features). Match your data format",defaultValue:!1},dropout:{description:"Dropout probability for regularization",advice:"0.0-0.5 typical. Higher for overfitting, lower for underfitting. 0 disables dropout",defaultValue:0},bidirectional:{description:"Whether to use bidirectional RNN",advice:"True for better context (2x parameters). False for causal/streaming applications",defaultValue:!1},nonlinearity:{description:"Activation function for RNN",advice:"'tanh' for standard RNN (vanishing gradients). 'relu' for better gradients but can explode",defaultValue:"tanh"},num_embeddings:{description:"Size of the vocabulary (number of unique tokens)",advice:"Set to your vocabulary size. Common: 1000-50000 depending on dataset",defaultValue:1e3},embedding_dim:{description:"Dimensionality of embedding vectors",advice:"Higher = more expressive. Common: 50-300 for small tasks, 512-1024 for large models",defaultValue:128},padding_idx:{description:"Index used for padding tokens",advice:"Usually 0. Embeddings at this index won't be updated during training",defaultValue:null},max_norm:{description:"Maximum norm for embedding vectors",advice:"Leave null for no constraint. Set to 1-10 to prevent embeddings from growing too large",defaultValue:null},norm_type:{description:"Type of norm to use for max_norm constraint",advice:"2.0 for L2 norm (standard). 1.0 for L1 norm. Only matters if max_norm is set",defaultValue:2},scale_grad_by_freq:{description:"Whether to scale gradients by word frequency",advice:"True helps rare words learn better. False for standard training",defaultValue:!1},sparse:{description:"Whether to use sparse gradients",advice:"True for memory efficiency with large vocabularies. False for standard dense updates",defaultValue:!1},p:{description:"Probability of dropout or data augmentation",advice:"0.0-1.0. Higher = more regularization/augmentation. Start with 0.5 for dropout, 0.5 for flips",defaultValue:.5},start_dim:{description:"First dimension to flatten",advice:"1 to preserve batch dimension. 0 to flatten everything including batch",defaultValue:1},end_dim:{description:"Last dimension to flatten",advice:"-1 to flatten till end. Positive numbers for specific end dimension",defaultValue:-1},ignore_index:{description:"Class index to ignore in loss calculation",advice:"Use for padding tokens or unknown classes. -100 is standard PyTorch ignore value",defaultValue:-100},reduction:{description:"Type of reduction to apply to loss",advice:"'mean' for average loss. 'sum' for total loss. 'none' for per-sample losses",defaultValue:"mean"},label_smoothing:{description:"Amount of label smoothing for regularization",advice:"0.0-0.1 typical. Higher = more smoothing = more regularization. 0 disables smoothing",defaultValue:0},pos_weight:{description:"Weight for positive class in binary classification",advice:"Use for imbalanced datasets. Higher = more weight on positive class. null for balanced",defaultValue:null},batch_size:{description:"Number of samples processed before updating weights",advice:"Larger = more stable gradients, more memory. Common: 16-128. GPU memory limited",defaultValue:32},epochs:{description:"Number of complete passes through the dataset",advice:"More epochs = more training. Watch for overfitting. Start with 10-100",defaultValue:10},device:{description:"Device to run training on",advice:"'auto' detects best. 'cuda' for GPU, 'cpu' for CPU, 'mps' for Apple Silicon",defaultValue:"auto"},num_workers:{description:"Number of processes for data loading",advice:"0 for single process. 2-8 for parallel loading. More workers = faster data loading",defaultValue:0},pin_memory:{description:"Whether to pin memory for faster GPU transfer",advice:"True for GPU training (faster). False for CPU-only or memory constraints",defaultValue:!0},shuffle:{description:"Whether to shuffle data each epoch",advice:"True for training (better generalization). False for validation/testing",defaultValue:!0},drop_last:{description:"Whether to drop the last incomplete batch",advice:"False to use all data. True for consistent batch sizes (important for some models)",defaultValue:!1},lr:{description:"Learning rate for parameter updates",advice:"Lower = slower, stable learning. Higher = faster, unstable. Start: 0.001 (Adam), 0.01 (SGD)",defaultValue:.001},weight_decay:{description:"L2 regularization strength",advice:"0 = no regularization. 1e-4 to 1e-2 typical. Higher = more regularization against overfitting",defaultValue:0},optim_eps:{description:"Small constant for numerical stability",advice:"1e-8 is standard. Rarely needs changing unless getting numerical issues",defaultValue:1e-8},amsgrad:{description:"Whether to use AMSGrad variant of Adam",advice:"False for standard Adam. True for better convergence on some problems",defaultValue:!1},sgd_momentum:{description:"Momentum factor for SGD",advice:"0 = no momentum. 0.9 typical. Higher = more momentum = smoother updates",defaultValue:0},dampening:{description:"Dampening factor for momentum in SGD",advice:"0 for standard momentum. >0 to reduce momentum effect",defaultValue:0},nesterov:{description:"Whether to use Nesterov momentum",advice:"False for standard momentum. True for Nesterov (often better convergence)",defaultValue:!1},rmsprop_alpha:{description:"Smoothing constant for RMSprop",advice:"0.99 is standard. Lower = more responsive to recent gradients. Higher = smoother",defaultValue:.99},centered:{description:"Whether to compute centered RMSprop",advice:"False for standard RMSprop. True for centered variant (can help convergence)",defaultValue:!1},degrees:{description:"Range of rotation angles in degrees",advice:"Small rotations (10-30\xb0) for natural images. Larger (90\xb0) if rotation invariance needed",defaultValue:30},interpolation:{description:"Interpolation method for transformations",advice:"'bilinear' for smooth results. 'nearest' for speed. 'bicubic' for highest quality",defaultValue:"bilinear"},expand:{description:"Whether to expand image to fit rotated content",advice:"False maintains size (crops). True shows full rotated image (changes dimensions)",defaultValue:!1},center:{description:"Center point for rotation",advice:"null for image center. Set (x,y) for custom rotation point",defaultValue:null},fill:{description:"Fill color for areas outside image after rotation",advice:"0 for black. 255 for white. Match your data preprocessing",defaultValue:0},size:{description:"Target size after random resized crop",advice:"Match your model's expected input size. Common: 224, 256, 512",defaultValue:224},scale_min:{description:"Minimum fraction of image area to crop",advice:"0.08 is standard. Lower = more aggressive cropping. Higher = less cropping",defaultValue:.08},scale_max:{description:"Maximum fraction of image area to crop",advice:"1.0 is standard (full image). Lower values force cropping",defaultValue:1},ratio_min:{description:"Minimum aspect ratio for cropping",advice:"0.75 is standard. Lower allows more extreme aspect ratios",defaultValue:.75},ratio_max:{description:"Maximum aspect ratio for cropping",advice:"1.33 is standard. Higher allows more extreme aspect ratios",defaultValue:1.33},brightness:{description:"Random brightness adjustment factor",advice:"0 = no change. 0.2 = \xb120% brightness. Higher = more variation",defaultValue:0},contrast:{description:"Random contrast adjustment factor",advice:"0 = no change. 0.2 = \xb120% contrast. Higher = more variation",defaultValue:0},saturation:{description:"Random saturation adjustment factor",advice:"0 = no change. 0.2 = \xb120% saturation. Higher = more color variation",defaultValue:0},hue:{description:"Random hue shift factor",advice:"0 = no change. \xb10.1 typical. Stay within \xb10.5 to avoid unrealistic colors",defaultValue:0},mean:{description:"Mean values for normalization per channel",advice:"Use dataset statistics or ImageNet values [0.485, 0.456, 0.406] for transfer learning",defaultValue:[.485,.456,.406]},std:{description:"Standard deviation values for normalization per channel",advice:"Use dataset statistics or ImageNet values [0.229, 0.224, 0.225] for transfer learning",defaultValue:[.229,.224,.225]},task:{description:"Type of classification task",advice:"'binary' for 2 classes. 'multiclass' for >2 mutually exclusive classes. 'multilabel' for multiple labels per sample",defaultValue:"multiclass"},num_classes:{description:"Number of classes in classification",advice:"Must match your dataset. 2 for binary, >2 for multiclass",defaultValue:10},top_k:{description:"Number of top predictions to consider for accuracy",advice:"1 for standard accuracy. 5 for top-5 accuracy (common for ImageNet)",defaultValue:1},average:{description:"Averaging method for multi-class metrics",advice:"'macro' for equal class weight. 'micro' for sample weight. 'weighted' for class frequency weight",defaultValue:"macro"},monitor:{description:"Metric to monitor for callbacks",advice:"'val_loss' for validation loss. 'val_accuracy' for validation accuracy. Choose based on your goal",defaultValue:"val_loss"},mode:{description:"Whether monitored metric should be minimized or maximized",advice:"'min' for loss metrics. 'max' for accuracy metrics",defaultValue:"min"},save_last:{description:"Whether to always save the last model",advice:"True to keep most recent model. False to only keep best",defaultValue:!1},verbose:{description:"Whether to print callback messages",advice:"True for debugging. False for cleaner output",defaultValue:!1},save_top_k:{description:"Number of best models to keep",advice:"1 to keep only best. -1 to keep all. Higher numbers for model ensembling",defaultValue:1},patience:{description:"Number of epochs to wait before stopping",advice:"Higher = more patience. 7-10 typical. Lower for quick training, higher for careful optimization",defaultValue:7},min_delta:{description:"Minimum change to qualify as improvement",advice:"0.0 for any improvement. Small positive values (0.001) to avoid noise",defaultValue:0},restore_best_weights:{description:"Whether to restore best weights when stopping",advice:"True to get best model. False to keep final weights",defaultValue:!1},logging_interval:{description:"How often to log learning rate",advice:"'epoch' for per-epoch logging. 'step' for per-batch logging (more detailed)",defaultValue:"epoch"},log_momentum:{description:"Whether to also log momentum values",advice:"False for cleaner logs. True for detailed optimizer monitoring",defaultValue:!1}};var z=a(4800);function S(e){return void 0!==e}function A(e){let{paramName:t,torchClass:a,children:r}=e,[o,s]=(0,i.useState)(!1),[l,d]=(0,i.useState)({}),c=i.useRef(null),u=function(e,t){if(j[e])return j[e];let a={alpha:["elu_alpha","rmsprop_alpha"],eps:["norm_eps","optim_eps"],momentum:["norm_momentum","sgd_momentum"]};if(a[e]){for(let n of a[e])if(t.includes("ELU")&&"elu_alpha"===n||t.includes("RMSprop")&&"rmsprop_alpha"===n||t.includes("BatchNorm")&&"norm_eps"===n||t.includes("LayerNorm")&&"norm_eps"===n||t.includes("GroupNorm")&&"norm_eps"===n||t.includes("InstanceNorm")&&"norm_eps"===n||t.includes("BatchNorm")&&"norm_momentum"===n||t.includes("InstanceNorm")&&"norm_momentum"===n||t.includes("SGD")&&"sgd_momentum"===n||t.includes("Adam")&&"optim_eps"===n||t.includes("RMSprop")&&"optim_eps"===n)return j[n];return j[a[e][0]]}return null}(t,a);return u?(0,n.jsxs)("div",{className:"relative",children:[(0,n.jsxs)("div",{ref:c,onMouseEnter:()=>{s(!0),c.current&&d({right:"320px",top:c.current.getBoundingClientRect().top+window.scrollY,transform:"none"})},onMouseLeave:()=>s(!1),className:"flex items-center gap-1 cursor-help",children:[r,(0,n.jsx)(z.__w,{className:"text-xs text-gray-400"})]}),o&&(0,n.jsxs)("div",{className:"fixed z-[10000] w-80 p-3 bg-gray-900 text-white text-xs rounded-lg shadow-lg",style:l,children:[(0,n.jsx)("div",{className:"font-semibold mb-1",children:t}),(0,n.jsx)("div",{className:"mb-2 text-gray-200",children:u.description}),(0,n.jsxs)("div",{className:"mb-2 text-blue-200",children:[(0,n.jsx)("span",{className:"font-medium",children:"Advice:"})," ",u.advice]}),(0,n.jsxs)("div",{className:"text-green-200",children:[(0,n.jsx)("span",{className:"font-medium",children:"Default:"})," ",Array.isArray(u.defaultValue)?"[".concat(u.defaultValue.join(", "),"]"):String(u.defaultValue)]}),(0,n.jsx)("div",{className:"absolute top-3 -right-1 w-2 h-2 bg-gray-900 transform rotate-45"})]})]}):(0,n.jsx)(n.Fragment,{children:r})}function L(e){let{nodeId:t,onClose:a}=e,{nodes:i,edges:o,setNodes:l,setEdges:d}=y(),c=i.find(e=>e.id===t);if(!c)return null;let u=c.data.isTraining?s.find(e=>e.torchClass===c.data.registryKey):r.find(e=>e.torchClass===c.data.registryKey),p=(e,a)=>l(n=>n.map(n=>n.id===t?{...n,data:{...n.data,params:{...n.data.params,[e]:a}}}:n)),m=e=>l(a=>a.map(a=>a.id===t?{...a,data:{...a.data,label:e}}:a)),h=()=>{l(e=>e.filter(e=>e.id!==t)),d(e=>e.filter(e=>e.source!==t&&e.target!==t)),a()};return S(u)||"input.dataset"===c.data.registryKey?"input.dataset"===c.data.registryKey?(0,n.jsx)(T.lG,{open:!0,onClose:a,className:"fixed inset-0 z-50 flex",children:(0,n.jsxs)(T.lG.Panel,{className:"ml-auto w-80 bg-white shadow-xl p-4 space-y-4 overflow-y-auto text-gray-800",children:[(0,n.jsxs)("div",{className:"flex justify-between items-center",children:[(0,n.jsx)(T.lG.Title,{className:"text-lg font-bold text-gray-900",children:"Dataset"}),(0,n.jsx)("button",{onClick:h,className:"p-2 text-red-600 hover:text-red-700 hover:bg-red-50 rounded-full transition-colors",title:"Delete Node",children:(0,n.jsx)(z.qbC,{})})]}),(0,n.jsx)("div",{className:"bg-blue-50 p-3 rounded-lg border-l-4 border-blue-200",children:(0,n.jsx)("p",{className:"text-sm text-blue-800",children:"Input dataset node that provides training data. Select from popular computer vision datasets like MNIST, CIFAR10, or ImageNet. This is typically the first node in your model graph."})}),(0,n.jsxs)("div",{className:"space-y-1",children:[(0,n.jsx)("label",{className:"text-sm font-semibold text-gray-900",children:"Name"}),(0,n.jsx)("input",{type:"text",value:c.data.label||"",onChange:e=>m(e.target.value),placeholder:"Dataset",className:"w-full rounded border px-2 py-1 text-gray-900"})]}),(0,n.jsxs)("div",{className:"space-y-1",children:[(0,n.jsx)("label",{className:"text-sm font-semibold text-gray-900",children:"Dataset"}),(0,n.jsxs)("select",{value:c.data.params.dataset,onChange:e=>p("dataset",e.target.value),className:"w-full rounded border px-2 py-1 text-gray-900",children:[(0,n.jsx)("option",{value:"torchvision.datasets.MNIST",children:"MNIST"}),(0,n.jsx)("option",{value:"torchvision.datasets.FashionMNIST",children:"Fashion MNIST"}),(0,n.jsx)("option",{value:"torchvision.datasets.CIFAR10",children:"CIFAR10"}),(0,n.jsx)("option",{value:"torchvision.datasets.CIFAR100",children:"CIFAR100"}),(0,n.jsx)("option",{value:"torchvision.datasets.ImageNet",children:"ImageNet"}),(0,n.jsx)("option",{value:"torchvision.datasets.SVHN",children:"SVHN"}),(0,n.jsx)("option",{value:"torchvision.datasets.EMNIST",children:"EMNIST"}),(0,n.jsx)("option",{value:"torchvision.datasets.KMNIST",children:"KMNIST"}),(0,n.jsx)("option",{value:"torchvision.datasets.QMNIST",children:"QMNIST"}),(0,n.jsx)("option",{value:"torchvision.datasets.STL10",children:"STL10"}),(0,n.jsx)("option",{value:"torchvision.datasets.CelebA",children:"CelebA"})]})]}),(0,n.jsx)("button",{onClick:a,className:"w-full bg-blue-600 text-white py-2 rounded",children:"Close"})]})}):S(u)?(0,n.jsx)(T.lG,{open:!0,onClose:a,className:"fixed inset-0 z-50 flex",children:(0,n.jsxs)(T.lG.Panel,{className:"ml-auto w-80 bg-white shadow-xl p-4 space-y-4 overflow-y-auto text-gray-800",children:[(0,n.jsxs)("div",{className:"flex justify-between items-center",children:[(0,n.jsx)(T.lG.Title,{className:"text-lg font-bold text-gray-900",children:u.friendly}),(0,n.jsxs)("div",{className:"flex gap-2",children:[(0,n.jsx)("button",{onClick:()=>{S(u)&&l(e=>e.map(e=>e.id===t?{...e,data:{...e.data,params:{...u.defaults}}}:e))},className:"p-2 text-blue-600 hover:text-blue-700 hover:bg-blue-50 rounded-full transition-colors",title:"Restore Default Parameters",children:(0,n.jsx)(z.EEI,{})}),(0,n.jsx)("button",{onClick:h,className:"p-2 text-red-600 hover:text-red-700 hover:bg-red-50 rounded-full transition-colors",title:"Delete Node",children:(0,n.jsx)(z.qbC,{})})]})]}),(0,n.jsx)("div",{className:"bg-blue-50 p-3 rounded-lg border-l-4 border-blue-200",children:(0,n.jsx)("p",{className:"text-sm text-blue-800",children:u.description})}),(0,n.jsxs)("div",{className:"space-y-1",children:[(0,n.jsx)("label",{className:"text-sm font-semibold text-gray-900",children:"Name"}),(0,n.jsx)("input",{type:"text",value:c.data.label||"",onChange:e=>m(e.target.value),placeholder:u.friendly,className:"w-full rounded border px-2 py-1 text-gray-900"})]}),u.params.map(e=>(0,n.jsxs)("div",{className:"space-y-1",children:[(0,n.jsx)(A,{paramName:e.name,torchClass:u.torchClass,children:(0,n.jsx)("label",{className:"text-sm font-semibold text-gray-900",children:e.name})}),"int"===e.type||"float"===e.type?(0,n.jsx)("input",{type:"number",min:e.min,max:e.max,step:"float"===e.type?.1:1,value:c.data.params[e.name],onChange:t=>p(e.name,"float"===e.type?parseFloat(t.target.value):parseInt(t.target.value)),className:"w-full rounded border px-2 py-1 text-gray-900"}):"bool"===e.type?(0,n.jsx)("input",{type:"checkbox",checked:c.data.params[e.name],onChange:t=>p(e.name,t.target.checked)}):"select"===e.type?(0,n.jsx)("select",{value:c.data.params[e.name],onChange:t=>p(e.name,t.target.value),className:"w-full rounded border px-2 py-1 text-gray-900",children:e.options.map(e=>(0,n.jsx)("option",{children:e},e))}):null]},e.name)),(0,n.jsx)("button",{onClick:a,className:"w-full bg-blue-600 text-white py-2 rounded",children:"Close"})]})}):null:null}var M=a(2323);let R={"torchvision.datasets.MNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.FashionMNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.CIFAR10":{shape:[3,32,32],type:"float32"},"torchvision.datasets.CIFAR100":{shape:[3,32,32],type:"float32"},"torchvision.datasets.ImageNet":{shape:[3,224,224],type:"float32"},"torchvision.datasets.SVHN":{shape:[3,32,32],type:"float32"},"torchvision.datasets.EMNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.KMNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.QMNIST":{shape:[1,28,28],type:"float32"},"torchvision.datasets.STL10":{shape:[3,96,96],type:"float32"},"torchvision.datasets.CelebA":{shape:[3,218,178],type:"float32"}};function E(e,t){let a=e.data.params;switch(e.data.registryKey){case"input.dataset":return l[e.data.params.dataset].shape;case"torch.add":case"torch.nn.BatchNorm1d":case"torch.nn.BatchNorm2d":case"torch.nn.LayerNorm":default:return t;case"torch.cat":let n=a.dim||1,i=[...t];return i[n]=2*i[n],i;case"torch.nn.Linear":return[a.out_features];case"torch.nn.Conv1d":return[a.out_channels,Math.floor((t[1]+2*a.padding-a.kernel_size)/a.stride+1)];case"torch.nn.Conv2d":return[a.out_channels,Math.floor((t[1]+2*a.padding-a.kernel_size)/a.stride+1),Math.floor((t[2]+2*a.padding-a.kernel_size)/a.stride+1)];case"torch.nn.Conv3d":return[a.out_channels,Math.floor((t[1]+2*a.padding-a.kernel_size)/a.stride+1),Math.floor((t[2]+2*a.padding-a.kernel_size)/a.stride+1),Math.floor((t[3]+2*a.padding-a.kernel_size)/a.stride+1)];case"torch.nn.MaxPool2d":case"torch.nn.AvgPool2d":return[t[0],Math.floor((t[1]-a.kernel_size)/a.stride+1),Math.floor((t[2]-a.kernel_size)/a.stride+1)];case"torch.nn.Flatten":return[t.reduce((e,t)=>e*t,1)]}}function I(e,t,a,n){if(!e||!t)return{inputShape:null,outputShape:null};let i=function e(t,a,n){let i=a.find(e=>e.id===t);if(!i)return null;if("input.dataset"===i.data.registryKey)return R[i.data.params.dataset].shape;let r=n.filter(e=>e.target===t);if(0===r.length)return null;if("torch.add"===i.data.registryKey){if(r.length<2)return null;let t=r.map(t=>{let i=e(t.source,a,n);if(!i)return null;let r=a.find(e=>e.id===t.source);return r?E(r,i):null});return t.some(e=>!e)||!t.every(e=>JSON.stringify(e)===JSON.stringify(t[0]))?null:t[0]}if("torch.cat"===i.data.registryKey){if(r.length<2)return null;let t=r.map(t=>{let i=e(t.source,a,n);if(!i)return null;let r=a.find(e=>e.id===t.source);return r?E(r,i):null});return t.some(e=>!e)?null:t[0]}let o=r[0],s=e(o.source,a,n);if(!s)return null;let l=a.find(e=>e.id===o.source);return l?E(l,s):null}(e.id,a,n);if(!i)return{inputShape:null,outputShape:null};let r=E(e,i);return{inputShape:i,outputShape:r}}function K(e){let t,{id:a,sourceX:i,sourceY:o,targetX:l,targetY:d,sourcePosition:c,targetPosition:u,source:p,target:m,selected:h,markerEnd:f,data:g}=e,y=(0,k.pk)(),b=(0,k.Yu)(),v=y.find(e=>e.id===p),x=y.find(e=>e.id===m),_=function(e,t,a,n){if(!e||!t)return{valid:!1,reason:"Missing nodes"};let i=e=>{if("input.dataset"===e.data.registryKey)return{outputType:"dataset",outputShape:e.data.shape||"[N, C, H, W]",inputType:void 0,inputShape:void 0,category:"DataAugmentation"};let t=e.data.isTraining?s.find(t=>t.torchClass===e.data.registryKey):r.find(t=>t.torchClass===e.data.registryKey);return t||{outputType:"unknown",outputShape:"",inputType:void 0,inputShape:void 0,category:"unknown"}},o=i(e),l=i(t),{outputShape:d}=I(e,t,a,n);if(!d)return{valid:!1,reason:"Cannot determine shape"};if(e.data.isTraining&&t.data.isTraining){let a="training.Config"===e.data.registryKey,n="Metrics"===l.category,i="Optimization"===l.category,r=o.outputType,s=o.outputTypes||[];if(a&&(n?r="prediction":i&&(r="trainable")),!(r===l.inputType||s.includes(l.inputType||"")))return{valid:!1,reason:"Type mismatch: ".concat(r," -> ").concat(l.inputType)};if("DataAugmentation"===l.category){if(!function(e){let t=(Array.isArray(e)?"[".concat(e.join(", "),"]"):e).replace(/[\[\]]/g,"").split(",").map(e=>e.trim());return(4===t.length||3===t.length)&&t.every(e=>"N"===e||"C"===e||"H"===e||"W"===e||!isNaN(parseInt(e)))}(d))return{valid:!1,reason:"Invalid image shape: ".concat(d)};let e=function(e){let t=(Array.isArray(e)?"[".concat(e.join(", "),"]"):e).replace(/[\[\]]/g,"").split(",").map(e=>e.trim()),[a,n,i]=4===t.length?t.slice(1):t,r=parseInt(a),o=parseInt(n),s=parseInt(i);return isNaN(r)||isNaN(o)||isNaN(s)?null:{channels:r,height:o,width:s}}(d);if(!e)return{valid:!1,reason:"Could not parse shape dimensions"};switch(t.data.registryKey){case"transforms.RandomRotation":case"transforms.RandomHorizontalFlip":case"transforms.RandomVerticalFlip":if(e.height<=1||e.width<=1)return{valid:!1,reason:"Transform ".concat(t.data.registryKey," requires 2D image data")};break;case"transforms.ColorJitter":if(1!==e.channels&&3!==e.channels)return{valid:!1,reason:"ColorJitter requires 1 or 3 channels, got ".concat(e.channels)};break;case"transforms.Normalize":try{let a=JSON.parse(t.data.params.mean||"[0.485, 0.456, 0.406]"),n=JSON.parse(t.data.params.std||"[0.229, 0.224, 0.225]");if(a.length!==e.channels||n.length!==e.channels)return{valid:!1,reason:"Normalize parameters don't match channel count"}}catch(e){return{valid:!1,reason:"Invalid Normalize parameters"}}break;case"transforms.RandomResizedCrop":if(e.height<t.data.params.size||e.width<t.data.params.size)return{valid:!1,reason:"Image too small for crop size"}}}return{valid:!0}}if(!r.find(e=>e.torchClass===t.data.registryKey))return{valid:!1,reason:"Invalid target layer"};switch(t.data.registryKey){case"torch.nn.Flatten":let c=t.data.params.start_dim||1,u=t.data.params.end_dim||-1;if(c>=d.length)return{valid:!1,reason:"start_dim (".concat(c,") must be less than input dimensions (").concat(d.length,")")};if(-1!==u&&u<c)return{valid:!1,reason:"end_dim (".concat(u,") must be greater than or equal to start_dim (").concat(c,")")};if(-1!==u&&u>=d.length)return{valid:!1,reason:"end_dim (".concat(u,") must be less than input dimensions (").concat(d.length,")")};break;case"torch.nn.Embedding":if(1!==d.length)return{valid:!1,reason:"Embedding layer expects 1D input of indices"};break;case"torch.nn.Linear":if(d.length>1)return{valid:!1,reason:"Linear layer expects 1D input"};if(d[0]!==t.data.params.in_features)return{valid:!1,reason:"Input features mismatch: ".concat(d[0]," ≠ ").concat(t.data.params.in_features)};break;case"torch.nn.Conv1d":if(2!==d.length)return{valid:!1,reason:"Conv1d expects 2D input (C,L)"};if(d[0]!==t.data.params.in_channels)return{valid:!1,reason:"Input channels mismatch: ".concat(d[0]," ≠ ").concat(t.data.params.in_channels)};if(d[1]<t.data.params.kernel_size)return{valid:!1,reason:"Input length (".concat(d[1],") must be >= kernel size (").concat(t.data.params.kernel_size,")")};break;case"torch.nn.Conv2d":if(3!==d.length)return{valid:!1,reason:"Conv2d expects 3D input (C,H,W)"};if(d[0]!==t.data.params.in_channels)return{valid:!1,reason:"Input channels mismatch: ".concat(d[0]," ≠ ").concat(t.data.params.in_channels)};if(d[1]<t.data.params.kernel_size||d[2]<t.data.params.kernel_size)return{valid:!1,reason:"Input dimensions (".concat(d[1],"x").concat(d[2],") must be >= kernel size (").concat(t.data.params.kernel_size,")")};break;case"torch.nn.Conv3d":if(4!==d.length)return{valid:!1,reason:"Conv3d expects 4D input (C,D,H,W)"};if(d[0]!==t.data.params.in_channels)return{valid:!1,reason:"Input channels mismatch: ".concat(d[0]," ≠ ").concat(t.data.params.in_channels)};if(d[1]<t.data.params.kernel_size||d[2]<t.data.params.kernel_size||d[3]<t.data.params.kernel_size)return{valid:!1,reason:"Input dimensions must be >= kernel size (".concat(t.data.params.kernel_size,")")};break;case"torch.nn.MaxPool1d":case"torch.nn.AvgPool1d":if(2!==d.length)return{valid:!1,reason:"".concat(t.data.registryKey.split(".").pop()," expects 2D input (C,L)")};if(d[1]<t.data.params.kernel_size)return{valid:!1,reason:"Input length (".concat(d[1],") must be >= kernel size (").concat(t.data.params.kernel_size,")")};break;case"torch.nn.MaxPool2d":case"torch.nn.AvgPool2d":if(3!==d.length)return{valid:!1,reason:"".concat(t.data.registryKey.split(".").pop()," expects 3D input (C,H,W)")};if(d[1]<t.data.params.kernel_size||d[2]<t.data.params.kernel_size)return{valid:!1,reason:"Input dimensions (".concat(d[1],"x").concat(d[2],") must be >= kernel size (").concat(t.data.params.kernel_size,")")};break;case"torch.nn.MaxPool3d":case"torch.nn.AvgPool3d":if(4!==d.length)return{valid:!1,reason:"".concat(t.data.registryKey.split(".").pop()," expects 4D input (C,D,H,W)")};if(d[1]<t.data.params.kernel_size||d[2]<t.data.params.kernel_size||d[3]<t.data.params.kernel_size)return{valid:!1,reason:"Input dimensions must be >= kernel size (".concat(t.data.params.kernel_size,")")};break;case"torch.nn.AdaptiveAvgPool2d":if(3!==d.length)return{valid:!1,reason:"AdaptiveAvgPool2d expects 3D input (C,H,W)"};break;case"torch.nn.BatchNorm1d":if(2!==d.length)return{valid:!1,reason:"BatchNorm1d expects 2D input (N,C) or 3D input (N,C,L)"};if(d[0]!==t.data.params.num_features)return{valid:!1,reason:"Number of features mismatch: ".concat(d[0]," ≠ ").concat(t.data.params.num_features)};break;case"torch.nn.BatchNorm2d":if(3!==d.length)return{valid:!1,reason:"BatchNorm2d expects 3D input (N,C,H,W)"};if(d[0]!==t.data.params.num_features)return{valid:!1,reason:"Number of features mismatch: ".concat(d[0]," ≠ ").concat(t.data.params.num_features)};break;case"torch.nn.LayerNorm":if(d.length<1)return{valid:!1,reason:"LayerNorm expects at least 1D input"};let p=d[d.length-1];if(p!==t.data.params.normalized_shape)return{valid:!1,reason:"Last dimension mismatch: ".concat(p," ≠ ").concat(t.data.params.normalized_shape)};break;case"torch.nn.GroupNorm":if(d.length<2)return{valid:!1,reason:"GroupNorm expects at least 2D input (N,C,*)"};if(d[0]!==t.data.params.num_channels)return{valid:!1,reason:"Number of channels mismatch: ".concat(d[0]," ≠ ").concat(t.data.params.num_channels)};if(t.data.params.num_channels%t.data.params.num_groups!=0)return{valid:!1,reason:"Number of channels (".concat(t.data.params.num_channels,") must be divisible by number of groups (").concat(t.data.params.num_groups,")")};break;case"torch.nn.LSTM":case"torch.nn.GRU":case"torch.nn.RNN":if(1!==d.length)return{valid:!1,reason:"".concat(t.data.registryKey.split(".").pop()," expects 1D input for feature size")};if(d[0]!==t.data.params.input_size)return{valid:!1,reason:"Input size mismatch: ".concat(d[0]," ≠ ").concat(t.data.params.input_size)};break;case"torch.nn.Softmax":if(t.data.params.dim>=d.length)return{valid:!1,reason:"Softmax dimension (".concat(t.data.params.dim,") must be less than input dimensions (").concat(d.length,")")}}return{valid:!0}}(v,x,y,b),w=(null==v?void 0:v.data.isTraining)||(null==x?void 0:x.data.isTraining)?"default":(null==g?void 0:g.type)||"default",N="#1e40af",C=2,T="arrowhead";"residual"===w?(N="#059669",C=2,t="5,5",T="residual-arrow"):"sum"===w&&(N="#dc2626",C=3,T="sum-arrow"),_.valid||(N="#dc2626"),h&&(C+=2);let j=(i+l)/2,z=(o+d)/2,S=(e,t,a,n)=>{let i={left:e-a/2,right:e+a/2,top:t,bottom:t+n};return y.some(e=>{if(!e.position)return!1;let t={left:e.position.x-20,right:e.position.x+200+20,top:e.position.y-20,bottom:e.position.y+60+20};return!(i.right<t.left||i.left>t.right||i.bottom<t.top||i.top>t.bottom)})},A=(()=>{for(let e=40;e<=200;e+=20){let t=z-e;if(!S(j,t,300,100))return{x:j,y:t,offset:e};let a=z+e-100;if(!S(j,a,300,100))return{x:j,y:a,offset:-e}}return{x:j,y:z-200,offset:200}})(),[L]=(0,k.Fp)({sourceX:i,sourceY:o,sourcePosition:c,targetX:l,targetY:d,targetPosition:u}),{inputShape:M,outputShape:E}=I(v,x,y,b),K="Unknown",D="Unknown";if(v)if("input.dataset"===v.data.registryKey){let e=R[v.data.params.dataset];K="".concat(v.data.label," [").concat(e.shape.join(", "),"]")}else{let e=r.find(e=>e.torchClass===v.data.registryKey);e&&M&&(K="".concat(e.friendly," [").concat(M.join(", "),"]"))}if(x&&E){let e=r.find(e=>e.torchClass===x.data.registryKey);e&&(D="".concat(e.friendly," [").concat(E.join(", "),"]"))}return(0,n.jsxs)("g",{children:[(0,n.jsx)("path",{d:L,strokeWidth:20,stroke:"transparent",fill:"none",style:{pointerEvents:"stroke",cursor:"pointer"}}),h&&(0,n.jsx)("path",{d:L,strokeWidth:C+4,stroke:N,strokeDasharray:t,opacity:.25,style:{filter:"blur(4px)",pointerEvents:"none"}}),(0,n.jsx)("path",{id:a,className:"react-flow__edge-path",d:L,strokeWidth:C,stroke:N,strokeDasharray:t,markerEnd:"url(#".concat(T,")"),style:{transition:"all 0.2s ease",pointerEvents:"none"}}),(!_.valid||h)&&(0,n.jsx)("foreignObject",{width:300,height:100,x:A.x-150,y:A.y,className:"edge-info-label",requiredExtensions:"http://www.w3.org/1999/xhtml",style:{position:"relative",zIndex:1e3},children:(0,n.jsxs)("div",{className:"\n              ".concat(_.valid?h?"bg-blue-50 p-2 rounded shadow-lg border border-blue-200":"":"bg-white p-2 rounded shadow-lg border border-red-200","\n              text-xs transform-gpu\n            "),style:{position:"absolute",left:"50%",transform:"translateX(-50%)",width:"fit-content",minWidth:"200px",maxWidth:"300px",transition:"all 0.2s ease-out"},children:[(0,n.jsxs)("div",{className:"font-mono",children:[(0,n.jsx)("span",{className:"text-gray-400",children:"in:"})," ",(0,n.jsx)("span",{className:"text-gray-900",children:K})]}),(0,n.jsxs)("div",{className:"font-mono",children:[(0,n.jsx)("span",{className:"text-gray-400",children:"out:"})," ",(0,n.jsx)("span",{className:"text-gray-900",children:D})]}),!_.valid&&(0,n.jsxs)("div",{className:"text-red-600 mt-1 font-semibold",children:["⚠️ ",_.reason]}),h&&_.valid&&(0,n.jsxs)("div",{className:"text-blue-600 mt-1 font-semibold",children:["\uD83D\uDCCC Edge Selected - ",w," connection"]})]})})]})}var D=a(8907);function P(e){let{code:t,onCodeChange:a}=e;return(0,n.jsx)("div",{className:"w-full h-full",children:(0,n.jsx)(D.KE,{height:"100%",defaultLanguage:"python",theme:"vs-dark",value:t,onChange:e=>{null==a||a(e||"")},options:{minimap:{enabled:!0},fontSize:14,lineNumbers:"on",rulers:[80],wordWrap:"on"}})})}let q=i.memo(e=>(0,n.jsx)(K,{...e,data:{type:"default"}})),V=i.memo(e=>(0,n.jsx)(K,{...e,data:{type:"residual"}})),F=i.memo(e=>(0,n.jsx)(K,{...e,data:{type:"sum"}})),U={default:i.memo(e=>{let{data:t}=e,a="#2563eb",i="#e0e7ff",o="Unnamed Node",l="border-gray-200";if(t.registryKey)if("input.dataset"===t.registryKey)o="Dataset",a="#ca8a04",i="#fef9c3";else if(t.isTraining){let e=s.find(e=>e.torchClass===t.registryKey);if(e)switch(o=e.friendly,e.category){case"DataAugmentation":a="#9333ea",i="#f3e8ff";break;case"Training":a="#059669",i="#d1fae5";break;case"Optimization":default:a="#2563eb",i="#e0e7ff";break;case"Metrics":a="#eab308",i="#fef9c3";break;case"Callbacks":a="#dc2626",i="#fee2e2"}}else{let e=r.find(e=>e.torchClass===t.registryKey);if(e)if(o=e.friendly,t.registryKey.startsWith("blocks."))l="border-2 border-dashed border-purple-500",a="#7c3aed",i="#f3f4f6";else switch(e.category){case"Input":a="#ca8a04",i="#fef9c3";break;case"Layers":default:a="#2563eb",i="#e0e7ff";break;case"Activations":a="#059669",i="#d1fae5";break;case"Pooling":a="#ea580c",i="#fff7ed";break;case"Normalization":a="#9333ea",i="#f3e8ff";break;case"Loss":a="#dc2626",i="#fee2e2";break;case"Utility":a="#64748b",i="#f1f5f9"}}return(0,n.jsxs)("div",{className:"relative",style:{minWidth:"150px"},children:[(0,n.jsx)(k.h7,{type:"target",position:k.yX.Left,style:{background:a,width:8,height:8}}),(0,n.jsx)("div",{className:"px-4 py-2 shadow-md rounded-md border ".concat(l),style:{background:i},children:(0,n.jsx)("div",{className:"flex items-center",children:(0,n.jsx)("div",{className:"ml-2",children:(0,n.jsx)("div",{className:"text-sm font-medium",style:{color:a},children:o})})})}),(0,n.jsx)(k.h7,{type:"source",position:k.yX.Right,style:{background:a,width:8,height:8}})]})})},W={default:q,residual:V,sum:F};function H(){let{nodes:e,edges:t,setNodes:a,setEdges:o,mode:l,setMode:d,modelNodes:c,modelEdges:u,trainingNodes:p,trainingEdges:m,code:h,setCode:f}=y(),{isAuthenticated:g,user:v}=(0,M.n)(),[x,_]=(0,i.useState)(null),[T,j]=(0,i.useState)(null),[S,A]=(0,i.useState)(""),{screenToFlowPosition:R}=(0,k.VH)(),[E,I]=(0,i.useState)(!1),[K,D]=(0,i.useState)(null),[q,V]=(0,i.useState)("default"),F=(0,i.useRef)({isActive:!1,timeoutId:null,cleanup:()=>{}});(0,i.useEffect)(()=>()=>{F.current.cleanup()},[]);let H=(0,i.useMemo)(()=>e.some(e=>"input.dataset"===e.data.registryKey),[e]);i.useEffect(()=>{let e=e=>{"Delete"===e.key&&(x?(a(e=>e.filter(e=>e.id!==x)),o(e=>e.filter(e=>e.source!==x&&e.target!==x)),_(null)):T&&(o(e=>e.filter(e=>e.id!==T)),j(null)))};return window.addEventListener("keydown",e),()=>window.removeEventListener("keydown",e)},[x,T,a,o]);let B=(0,i.useCallback)(e=>{e.preventDefault();let t=e.dataTransfer.getData("application/reactflow");if(!t)return;let n=JSON.parse(t),i=!!n.isTraining;if("training"===l&&!i||"model"===l&&i)return;let o=i?s.find(e=>e.torchClass===n.registryKey):r.find(e=>e.torchClass===n.registryKey);if(!o&&"input.dataset"!==n.registryKey)return;let d=R({x:e.clientX,y:e.clientY}),c=o?o.torchClass:"input.dataset",u=(0,b.Ak)(6),p={id:u,type:"default",position:d,data:{registryKey:c,label:u,params:o?{...o.defaults}:{dataset:"torchvision.datasets.MNIST"},isTraining:i}};a(e=>[...e,p])},[R,a,l]),O=(0,i.useMemo)(()=>t.map(e=>{let t=e.type||"default",a="#1e40af",n=2;return"residual"===t?a="#059669":"sum"===t&&(a="#dc2626",n=3),{...e,markerEnd:{type:k.TG.Arrow,width:25,height:25,color:a},style:{strokeWidth:n,stroke:a,..."residual"===t&&{strokeDasharray:"5,5"}}}}),[t]),G=(0,i.useCallback)(e=>{if(!y.getState().isValidConnection(e))return void console.log("Invalid connection");let t={...e,type:"training"===l?"default":q};o(e=>(0,k.rN)(t,e))},[o,q,l]),J=(0,i.useCallback)((e,t)=>{let{nodeId:n}=t,i=y.getState();if(!i.nodes.find(e=>e.id===n))return;let r=i.nodes.filter(e=>e.id!==n&&i.isValidConnection({source:n,target:e.id,sourceHandle:"output",targetHandle:"input"}));a(e=>e.map(e=>({...e,className:r.some(t=>t.id===e.id)?"valid-target":""})))},[a]),X=(0,i.useCallback)(()=>{a(e=>e.map(e=>({...e,className:""})))},[a]),$=(0,i.useMemo)(()=>e.filter(e=>e.position&&"number"==typeof e.position.x&&"number"==typeof e.position.y),[e]),Y=(0,i.useCallback)((e,t)=>{j(t.id),_(null),V(t.type||"default")},[]),Z=(0,i.useCallback)((e,t)=>{E?K?K!==t.id&&(o(e=>(0,k.rN)({source:K,target:t.id},e)),D(null)):D(t.id):(_(t.id),j(null))},[E,K,o]),Q=(0,i.useCallback)(()=>{let e=e=>"boolean"==typeof e?e?"True":"False":null==e?"None":"string"==typeof e&&e.startsWith("[")&&e.endsWith("]")?e:Array.isArray(e)?"[".concat(e.map(e=>"string"==typeof e?JSON.stringify(e):e).join(", "),"]"):JSON.stringify(e),t=e=>e.data.label&&e.data.label!==e.data.registryKey?e.data.label.toLowerCase().replace(/[^a-z0-9]/g,"_").replace(/^[^a-z]/,"n$&").replace(/^$/,e.id):e.id,a=(t,a)=>{let n={...t.data.params};return"transforms.RandomResizedCrop"===a.torchClass&&(void 0!==n.scale_min&&void 0!==n.scale_max&&(n.scale=[n.scale_min,n.scale_max],delete n.scale_min,delete n.scale_max),void 0!==n.ratio_min&&void 0!==n.ratio_max&&(n.ratio=[n.ratio_min,n.ratio_max],delete n.ratio_min,delete n.ratio_max)),Object.entries(n).filter(e=>{let[t,a]=e;return null!=a}).map(t=>{let[a,n]=t;return"".concat(a,"=").concat(e(n))}).join(", ")},n=c.find(e=>"input.dataset"===e.data.registryKey);if(p.find(e=>"input.dataset"===e.data.registryKey),!n)return"# Error: No dataset node found in model.";let i=new Set,o=[],l=[n.id];for(;l.length>0;){let e=l.shift();if(i.has(e))continue;i.add(e);let t=c.find(t=>t.id===e);for(let a of(t&&!t.data.registryKey.includes("Loss")&&"input.dataset"!==t.data.registryKey&&o.push(t),u.filter(t=>t.source===e)))i.has(a.target)||l.push(a.target)}let d=c.find(e=>e.data.registryKey.includes("Loss")),m=p.filter(e=>{let t=s.find(t=>t.torchClass===e.data.registryKey);return null==t?void 0:t.torchClass.startsWith("transforms.")}),h=p.filter(e=>{let t=s.find(t=>t.torchClass===e.data.registryKey);return null==t?void 0:t.torchClass.startsWith("torch.optim.")}),f=p.filter(e=>{let t=s.find(t=>t.torchClass===e.data.registryKey);return null==t?void 0:t.torchClass.startsWith("metrics.")}),g=p.filter(e=>{let t=s.find(t=>t.torchClass===e.data.registryKey);return null==t?void 0:t.torchClass.startsWith("callbacks.")}),y=p.find(e=>"training.Config"===e.data.registryKey),b=(e,t,a,n)=>{let i=r.find(t=>t.torchClass===e.data.registryKey);if(!i)return{channels:t};let o=e.data.params;if("torch.nn.AdaptiveAvgPool2d"===i.torchClass||"torch.nn.AdaptiveMaxPool2d"===i.torchClass){let e=o.output_size||1;return{channels:t,height:e,width:e}}if("torch.nn.Flatten"===i.torchClass)return 1===(o.start_dim||1)&&a&&n?{channels:t,flattened:t*a*n}:{channels:t,flattened:t};if("torch.nn.Linear"===i.torchClass)return{channels:o.out_features||t};if(i.torchClass.includes("Conv"))return{channels:o.out_channels||t};if(i.torchClass.startsWith("blocks.ResNet"))return{channels:o.out_channels||t};else if("blocks.InceptionModule"===i.torchClass)return{channels:(o.out_1x1||0)+(o.out_3x3||0)+(o.out_5x5||0)+(o.out_pool||0)};return{channels:t,height:a,width:n}},v=new Map;if(n){var x;let e=(null==(x=n.data.params)?void 0:x.dataset)||"torchvision.datasets.MNIST";v.set(n.id,{"torchvision.datasets.MNIST":{channels:1,height:28,width:28},"torchvision.datasets.FashionMNIST":{channels:1,height:28,width:28},"torchvision.datasets.EMNIST":{channels:1,height:28,width:28},"torchvision.datasets.KMNIST":{channels:1,height:28,width:28},"torchvision.datasets.QMNIST":{channels:1,height:28,width:28},"torchvision.datasets.CIFAR10":{channels:3,height:32,width:32},"torchvision.datasets.CIFAR100":{channels:3,height:32,width:32},"torchvision.datasets.ImageNet":{channels:3,height:224,width:224},"torchvision.datasets.SVHN":{channels:3,height:32,width:32},"torchvision.datasets.STL10":{channels:3,height:96,width:96},"torchvision.datasets.CelebA":{channels:3,height:218,width:178}}[e]||{channels:3,height:224,width:224})}o.forEach(e=>{let t=u.filter(t=>t.target===e.id);if(t.length>0){let a=t[0].source,n=v.get(a);if(n){let t=b(e,n.channels,n.height,n.width),a=r.find(t=>t.torchClass===e.data.registryKey);a&&(a.torchClass.includes("Conv")||"torch.nn.BatchNorm2d"===a.torchClass?a.torchClass.includes("Conv")?e.data.params.in_channels=n.channels:"torch.nn.BatchNorm2d"===a.torchClass&&(e.data.params.num_features=n.channels):"torch.nn.Linear"===a.torchClass?void 0!==n.flattened?e.data.params.in_features=n.flattened:n.height&&n.width?e.data.params.in_features=n.channels*n.height*n.width:e.data.params.in_features=n.channels:(a.torchClass.startsWith("blocks.ResNet")||"blocks.InceptionModule"===a.torchClass)&&(e.data.params.in_channels=n.channels)),v.set(e.id,t)}}});let _="";_+="import torch\n",_+="import torch.nn as nn\n",_+="import torch.optim as optim\n",_+="from torch.utils.data import DataLoader\n",_+="from torchvision import datasets, transforms\n",f.length>0&&(_+="from torchmetrics import Accuracy, Precision, Recall, F1Score\n"),g.length>0&&(_+="from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n"),_+="\n";let k=o.some(e=>{var t;return null==(t=e.data.registryKey)?void 0:t.startsWith("blocks.ResNet")}),w=o.some(e=>{var t;return null==(t=e.data.registryKey)?void 0:t.startsWith("blocks.Inception")});k&&(_+="# ResNet Block Definitions\n",_+="class ResNetBasicBlock(nn.Module):\n",_+="    def __init__(self, in_channels, out_channels, stride=1, downsample=False):\n",_+="        super().__init__()\n",_+="        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",_+="        self.bn1 = nn.BatchNorm2d(out_channels)\n",_+="        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",_+="        self.bn2 = nn.BatchNorm2d(out_channels)\n",_+="        self.relu = nn.ReLU(inplace=True)\n",_+="        self.downsample = None\n",_+="        if downsample or stride != 1 or in_channels != out_channels:\n",_+="            self.downsample = nn.Sequential(\n",_+="                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",_+="                nn.BatchNorm2d(out_channels)\n",_+="            )\n\n",_+="    def forward(self, x):\n",_+="        identity = x\n",_+="        out = self.conv1(x)\n",_+="        out = self.bn1(out)\n",_+="        out = self.relu(out)\n",_+="        out = self.conv2(out)\n",_+="        out = self.bn2(out)\n",_+="        if self.downsample is not None:\n",_+="            identity = self.downsample(x)\n",_+="        out += identity\n",_+="        out = self.relu(out)\n",_+="        return out\n\n",_+="class ResNetBottleneckBlock(nn.Module):\n",_+="    def __init__(self, in_channels, out_channels, stride=1, downsample=False):\n",_+="        super().__init__()\n",_+="        expansion = 4\n",_+="        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",_+="        self.bn1 = nn.BatchNorm2d(out_channels)\n",_+="        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride, 1, bias=False)\n",_+="        self.bn2 = nn.BatchNorm2d(out_channels)\n",_+="        self.conv3 = nn.Conv2d(out_channels, out_channels * expansion, 1, bias=False)\n",_+="        self.bn3 = nn.BatchNorm2d(out_channels * expansion)\n",_+="        self.relu = nn.ReLU(inplace=True)\n",_+="        self.downsample = None\n",_+="        if downsample or stride != 1 or in_channels != out_channels * expansion:\n",_+="            self.downsample = nn.Sequential(\n",_+="                nn.Conv2d(in_channels, out_channels * expansion, 1, stride, bias=False),\n",_+="                nn.BatchNorm2d(out_channels * expansion)\n",_+="            )\n\n",_+="    def forward(self, x):\n",_+="        identity = x\n",_+="        out = self.conv1(x)\n",_+="        out = self.bn1(out)\n",_+="        out = self.relu(out)\n",_+="        out = self.conv2(out)\n",_+="        out = self.bn2(out)\n",_+="        out = self.relu(out)\n",_+="        out = self.conv3(out)\n",_+="        out = self.bn3(out)\n",_+="        if self.downsample is not None:\n",_+="            identity = self.downsample(x)\n",_+="        out += identity\n",_+="        out = self.relu(out)\n",_+="        return out\n\n"),w&&(_+="# Inception Block Definition\n",_+="class InceptionModule(nn.Module):\n",_+="    def __init__(self, in_channels, out_1x1, out_3x3_reduce, out_3x3, out_5x5_reduce, out_5x5, out_pool):\n",_+="        super().__init__()\n",_+="        # 1x1 conv branch\n",_+="        self.branch1 = nn.Conv2d(in_channels, out_1x1, 1)\n",_+="        # 3x3 conv branch\n",_+="        self.branch2 = nn.Sequential(\n",_+="            nn.Conv2d(in_channels, out_3x3_reduce, 1),\n",_+="            nn.ReLU(inplace=True),\n",_+="            nn.Conv2d(out_3x3_reduce, out_3x3, 3, padding=1)\n",_+="        )\n",_+="        # 5x5 conv branch\n",_+="        self.branch3 = nn.Sequential(\n",_+="            nn.Conv2d(in_channels, out_5x5_reduce, 1),\n",_+="            nn.ReLU(inplace=True),\n",_+="            nn.Conv2d(out_5x5_reduce, out_5x5, 5, padding=2)\n",_+="        )\n",_+="        # Pooling branch\n",_+="        self.branch4 = nn.Sequential(\n",_+="            nn.MaxPool2d(3, stride=1, padding=1),\n",_+="            nn.Conv2d(in_channels, out_pool, 1)\n",_+="        )\n\n",_+="    def forward(self, x):\n",_+="        branch1 = self.branch1(x)\n",_+="        branch2 = self.branch2(x)\n",_+="        branch3 = self.branch3(x)\n",_+="        branch4 = self.branch4(x)\n",_+="        return torch.cat([branch1, branch2, branch3, branch4], 1)\n\n"),_+="# Data transforms\n",_+="transform = transforms.Compose([\n",_+="    transforms.ToTensor(),\n",m.forEach(e=>{let t=s.find(t=>t.torchClass===e.data.registryKey);if(t){let n=t.torchClass.split(".").pop()||"",i=a(e,t);_+="    transforms.".concat(n,"(").concat(i,"),\n")}}),_+="])\n\n";let N=(n.data.params.dataset||"torchvision.datasets.MNIST").split(".").pop();_+="# Dataset\n",_+="train_dataset = datasets.".concat(N,"(root='./data', train=True, download=True, transform=transform)\n"),_+="test_dataset = datasets.".concat(N,"(root='./data', train=False, download=True, transform=transform)\n\n");let C=(null==y?void 0:y.data.params.batch_size)||32,T=(null==y?void 0:y.data.params.num_workers)||4,j=(null==y?void 0:y.data.params.pin_memory)!==!1;_+="# Data loaders\n",_+="train_loader = DataLoader(train_dataset, batch_size=".concat(C,", shuffle=True, num_workers=").concat(T,", pin_memory=").concat(j?"True":"False",")\n"),_+="test_loader = DataLoader(test_dataset, batch_size=".concat(C,", shuffle=False, num_workers=").concat(T,", pin_memory=").concat(j?"True":"False",")\n\n"),_+="# Model definition\n",_+="class Net(nn.Module):\n",_+="    def __init__(self):\n",_+="        super().__init__()\n",o.forEach(e=>{let n=r.find(t=>t.torchClass===e.data.registryKey);if(n&&e.data.registryKey.startsWith("torch.nn.")){let i=a(e,n),r=n.torchClass.split(".").pop();_+="        self.".concat(t(e)," = nn.").concat(r,"(").concat(i,")\n")}else if(n&&e.data.registryKey.startsWith("blocks.")){let i=a(e,n),r=n.torchClass.split(".").pop();_+="        self.".concat(t(e)," = ").concat(r,"(").concat(i,")\n")}}),_+="\n",_+="    def forward(self, x):\n";let z=new Set,S=new Map,A=new Map,L=new Map;u.forEach(e=>{"residual"===e.type?(A.has(e.target)||A.set(e.target,[]),A.get(e.target).push(e.source)):"sum"===e.type&&(L.has(e.target)||L.set(e.target,[]),L.get(e.target).push(e.source))}),o.forEach((e,a)=>{let n=t(e),i=u.filter(t=>t.target===e.id),r=A.has(e.id),s=L.has(e.id);if("torch.add"===e.data.registryKey){if(i.length>=2){let e=i.map(e=>{let a=c.find(t=>t.id===e.source),n=a?t(a):"x";return S.get(n)||(n===t(o[0])?"x":n)}).join(" + ");_+="        ".concat(n,"_out = ").concat(e,"\n"),S.set(n,"".concat(n,"_out"))}}else if("torch.cat"===e.data.registryKey){if(i.length>=2){let a=i.map(e=>{let a=c.find(t=>t.id===e.source),n=a?t(a):"x";return S.get(n)||(n===t(o[0])?"x":n)}).join(", "),r=e.data.params.dim||1;_+="        ".concat(n,"_out = torch.cat([").concat(a,"], dim=").concat(r,")\n"),S.set(n,"".concat(n,"_out"))}}else{let l=i.find(e=>!e.type||"default"===e.type),d="x";if(l){let e=c.find(e=>e.id===l.source);if(e&&"input.dataset"!==e.data.registryKey){let a=t(e);d=S.get(a)||"".concat(a,"_out")}}else if(i.length>0){let e=c.find(e=>e.id===i[0].source);if(e&&"input.dataset"!==e.data.registryKey){let a=t(e);d=S.get(a)||"".concat(a,"_out")}}else if(a>0){let e=t(o[a-1]);d=S.get(e)||"".concat(e,"_out")}if(e.data.registryKey.startsWith("torch.nn.")||e.data.registryKey.startsWith("blocks.")?_+="        ".concat(n,"_out = self.").concat(n,"(").concat(d,")\n"):"torch.add"===e.data.registryKey?_+="        ".concat(n,"_out = ").concat(d,"  # torch.add will be handled by sum connections\n"):_+="        ".concat(n,"_out = ").concat(d,"  # ").concat(e.data.registryKey,"\n"),s){let a=L.get(e.id).map(e=>{let a=c.find(t=>t.id===e),n=a?t(a):"x";return S.get(n)||(n===t(o[0])?"x":n)}).join(" + ");_+="        ".concat(n,"_out = ").concat(n,"_out + ").concat(a,"  # Sum connection\n")}if(r){let a=A.get(e.id).map(e=>{let a=c.find(t=>t.id===e),n=a?t(a):"x";return S.get(n)||(n===t(o[0])?"x":n)}).join(" + ");_+="        ".concat(n,"_out = ").concat(n,"_out + ").concat(a,"  # Residual connection\n")}S.set(n,"".concat(n,"_out"))}z.add(e.id)});let M=t(o[o.length-1]),R=S.get(M)||"".concat(M,"_out");_+="        return ".concat(R,"\n\n"),_+="# Training setup\n";let E=(null==y?void 0:y.data.params.device)||"cuda";if(_+='device = torch.device("'.concat(E,'" if torch.cuda.is_available() else "cpu")\n'),_+="model = Net().to(device)\n",h.length>0){let t=h[0],a=s.find(e=>e.torchClass===t.data.registryKey);if(a){let n={...t.data.params};"torch.optim.Adam"!==a.torchClass||n.betas||(n.betas=[.9,.999]);let i=Object.entries(n).filter(e=>{let[t,a]=e;return null!=a}).map(t=>{let[a,n]=t;return"".concat(a,"=").concat(e(n))}).join(", "),r=a.torchClass.split(".").pop();_+="optimizer = optim.".concat(r,"(model.parameters(), ").concat(i,")\n")}}else _+="optimizer = optim.Adam(model.parameters(), lr=0.001)\n";if(d){let e=r.find(e=>e.torchClass===d.data.registryKey);if(e){let t=a(d,e),n=e.torchClass.split(".").pop();_+="criterion = nn.".concat(n,"(").concat(t,")\n\n")}}else _+="criterion = nn.CrossEntropyLoss()\n\n";f.length>0&&(_+="# Metrics\n",f.forEach(a=>{let n=s.find(e=>e.torchClass===a.data.registryKey);if(n){let i=n.torchClass.split(".").pop()||"",r={...a.data.params};["Accuracy","Precision","Recall","F1Score"].includes(i)&&!r.task&&(r.task="multiclass",r.num_classes||(r.num_classes=10));let o=Object.entries(r).filter(e=>{let[t,a]=e;return null!=a}).map(t=>{let[a,n]=t;return"".concat(a,"=").concat(e(n))}).join(", ");_+="".concat(t(a)," = ").concat(i,"(").concat(o,").to(device)\n")}}),_+="\n"),g.length>0&&(_+="# Callbacks\n",g.forEach(e=>{let n=s.find(t=>t.torchClass===e.data.registryKey);if(n){let i=n.torchClass.split(".").pop()||"",r=a(e,n);_+="".concat(t(e)," = ").concat(i,"(").concat(r,")\n")}}),_+="\n"),_+="# Training loop\n";let I=(null==y?void 0:y.data.params.epochs)||10;return _+="num_epochs = ".concat(I,"\n\n"),_+="def train_epoch():\n",_+="    model.train()\n",_+="    running_loss = 0.0\n",_+="    for batch_idx, (data, target) in enumerate(train_loader):\n",_+="        data, target = data.to(device), target.to(device)\n",_+="        optimizer.zero_grad()\n",_+="        output = model(data)\n",_+="        loss = criterion(output, target)\n",_+="        loss.backward()\n",_+="        optimizer.step()\n",_+="        running_loss += loss.item()\n",_+="        if batch_idx % 100 == 0:\n",_+='            print(f"Train Batch {batch_idx}/{len(train_loader)} "\n',_+='                  f"Loss: {running_loss / (batch_idx + 1):.6f}")\n\n',_+="def evaluate():\n",_+="    model.eval()\n",_+="    test_loss = 0\n",f.length>0&&f.forEach(e=>{_+="    ".concat(t(e),".reset()\n")}),_+="    with torch.no_grad():\n",_+="        for data, target in test_loader:\n",_+="            data, target = data.to(device), target.to(device)\n",_+="            output = model(data)\n",_+="            test_loss += criterion(output, target).item()\n",f.length>0&&f.forEach(e=>{s.find(t=>t.torchClass===e.data.registryKey)&&(_+="            ".concat(t(e),".update(output, target)\n"))}),_+="    test_loss /= len(test_loader)\n",_+='    print(f"Test set: Average loss: {test_loss:.4f}")\n',f.length>0&&f.forEach(e=>{let a=s.find(t=>t.torchClass===e.data.registryKey);if(a){let n=a.torchClass.split(".").pop();_+='    print(f"Test set: '.concat(n,": {").concat(t(e),'.compute():.4f}")\n')}}),_+="\n",_+="def train():\n",_+='    best_metric = float("inf")\n',_+="    for epoch in range(num_epochs):\n",_+='        print(f"Epoch {epoch+1}/{num_epochs}")\n',_+="        train_epoch()\n",_+="        evaluate()\n",g.length>0&&g.forEach(e=>{let t=s.find(t=>t.torchClass===e.data.registryKey);t&&"callbacks.ModelCheckpoint"===t.torchClass?(_+="        # Model checkpoint\n",_+="        if test_loss < best_metric:\n",_+="            best_metric = test_loss\n",_+='            torch.save(model.state_dict(), "best_model.pth")\n'):t&&"callbacks.EarlyStopping"===t.torchClass?_+="        # Early stopping logic would go here\n":t&&"callbacks.LearningRateMonitor"===t.torchClass&&(_+="        # Learning rate monitoring would go here\n")}),_+="\n",_+='if __name__ == "__main__":\n',_+="    train()\n"},[c,u,p,m]);return(0,i.useCallback)(()=>{let e=[],t={id:(0,b.Ak)(6),type:"default",position:{x:100,y:100},data:{registryKey:"torch.nn.Conv2d",label:"input_conv",params:{in_channels:3,out_channels:64,kernel_size:7,stride:2,padding:3,bias:!1},isTraining:!1}};e.push(t);let n={id:(0,b.Ak)(6),type:"default",position:{x:300,y:100},data:{registryKey:"torch.nn.BatchNorm2d",label:"input_bn",params:{num_features:64},isTraining:!1}};e.push(n);let i={id:(0,b.Ak)(6),type:"default",position:{x:500,y:100},data:{registryKey:"torch.nn.ReLU",label:"input_relu",params:{inplace:!0},isTraining:!1}};e.push(i);let r={id:(0,b.Ak)(6),type:"default",position:{x:700,y:100},data:{registryKey:"torch.nn.MaxPool2d",label:"input_pool",params:{kernel_size:3,stride:2,padding:1},isTraining:!1}};e.push(r),[{x:100,y:200},{x:300,y:200},{x:500,y:200}].forEach((t,a)=>{let n={id:(0,b.Ak)(6),type:"default",position:t,data:{registryKey:"blocks.ResNetBasicBlock",label:"resblock_".concat(a+1),params:{in_channels:64,out_channels:64,stride:1,downsample:!1},isTraining:!1}};e.push(n)});let s={id:(0,b.Ak)(6),type:"default",position:{x:700,y:200},data:{registryKey:"torch.nn.AdaptiveAvgPool2d",label:"global_pool",params:{output_size:1},isTraining:!1}};e.push(s);let l={id:(0,b.Ak)(6),type:"default",position:{x:900,y:200},data:{registryKey:"torch.nn.Flatten",label:"flatten",params:{start_dim:1},isTraining:!1}};e.push(l);let d={id:(0,b.Ak)(6),type:"default",position:{x:1100,y:200},data:{registryKey:"torch.nn.Linear",label:"classifier",params:{in_features:64,out_features:10},isTraining:!1}};e.push(d);let c=[{id:"".concat(t.id,"-").concat(n.id),source:t.id,target:n.id,type:"default"},{id:"".concat(n.id,"-").concat(i.id),source:n.id,target:i.id,type:"default"},{id:"".concat(i.id,"-").concat(r.id),source:i.id,target:r.id,type:"default"},{id:"".concat(r.id,"-").concat(e[4].id),source:r.id,target:e[4].id,type:"default"},{id:"".concat(e[4].id,"-").concat(e[5].id),source:e[4].id,target:e[5].id,type:"default"},{id:"".concat(e[5].id,"-").concat(e[6].id),source:e[5].id,target:e[6].id,type:"default"},{id:"".concat(e[6].id,"-").concat(s.id),source:e[6].id,target:s.id,type:"default"},{id:"".concat(s.id,"-").concat(l.id),source:s.id,target:l.id,type:"default"},{id:"".concat(l.id,"-").concat(d.id),source:l.id,target:d.id,type:"default"}];a(t=>[...t,...e]),o(e=>[...e,...c])},[a,o]),(0,i.useCallback)(()=>{let e=[],t={id:(0,b.Ak)(6),type:"default",position:{x:100,y:100},data:{registryKey:"torch.nn.Conv2d",label:"conv1",params:{in_channels:3,out_channels:64,kernel_size:7,stride:2,padding:3,bias:!1},isTraining:!1}};e.push(t);let n={id:(0,b.Ak)(6),type:"default",position:{x:350,y:100},data:{registryKey:"torch.nn.MaxPool2d",label:"maxpool1",params:{kernel_size:3,stride:2,padding:1},isTraining:!1}};e.push(n);let i={id:(0,b.Ak)(6),type:"default",position:{x:100,y:220},data:{registryKey:"blocks.InceptionModule",label:"inception1",params:{in_channels:64,out_1x1:32,out_3x3_reduce:48,out_3x3:64,out_5x5_reduce:8,out_5x5:16,out_pool:16},isTraining:!1}};e.push(i);let r={id:(0,b.Ak)(6),type:"default",position:{x:350,y:220},data:{registryKey:"blocks.InceptionModule",label:"inception2",params:{in_channels:128,out_1x1:64,out_3x3_reduce:96,out_3x3:128,out_5x5_reduce:16,out_5x5:32,out_pool:32},isTraining:!1}};e.push(r);let s={id:(0,b.Ak)(6),type:"default",position:{x:600,y:220},data:{registryKey:"torch.nn.AdaptiveAvgPool2d",label:"global_pool",params:{output_size:1},isTraining:!1}};e.push(s);let l={id:(0,b.Ak)(6),type:"default",position:{x:850,y:220},data:{registryKey:"torch.nn.Flatten",label:"flatten",params:{start_dim:1},isTraining:!1}};e.push(l);let d={id:(0,b.Ak)(6),type:"default",position:{x:1100,y:220},data:{registryKey:"torch.nn.Linear",label:"classifier",params:{in_features:256,out_features:10},isTraining:!1}};e.push(d);let c=[{id:"".concat(t.id,"-").concat(n.id),source:t.id,target:n.id,type:"default"},{id:"".concat(n.id,"-").concat(i.id),source:n.id,target:i.id,type:"default"},{id:"".concat(i.id,"-").concat(r.id),source:i.id,target:r.id,type:"default"},{id:"".concat(r.id,"-").concat(s.id),source:r.id,target:s.id,type:"default"},{id:"".concat(s.id,"-").concat(l.id),source:s.id,target:l.id,type:"default"},{id:"".concat(l.id,"-").concat(d.id),source:l.id,target:d.id,type:"default"}];a(t=>[...t,...e]),o(e=>[...e,...c])},[a,o]),(0,i.useCallback)(()=>{let e=[],t=[],n={id:(0,b.Ak)(6),type:"default",position:{x:50,y:50},data:{registryKey:"input.dataset",label:"dataset",params:{dataset:"torchvision.datasets.CIFAR10"},isTraining:!1}};e.push(n);let i={id:(0,b.Ak)(6),type:"default",position:{x:230,y:50},data:{registryKey:"torch.nn.Conv2d",label:"conv1",params:{in_channels:3,out_channels:64,kernel_size:3,stride:1,padding:1,bias:!1},isTraining:!1}};e.push(i);let r={id:(0,b.Ak)(6),type:"default",position:{x:410,y:50},data:{registryKey:"blocks.ResNetBasicBlock",label:"resblock1",params:{in_channels:64,out_channels:64,stride:1,downsample:!1},isTraining:!1}};e.push(r);let s={id:(0,b.Ak)(6),type:"default",position:{x:590,y:50},data:{registryKey:"torch.nn.AdaptiveAvgPool2d",label:"global_pool",params:{output_size:1},isTraining:!1}};e.push(s);let l={id:(0,b.Ak)(6),type:"default",position:{x:770,y:50},data:{registryKey:"torch.nn.Flatten",label:"flatten",params:{start_dim:1},isTraining:!1}};e.push(l);let d={id:(0,b.Ak)(6),type:"default",position:{x:950,y:50},data:{registryKey:"torch.nn.Linear",label:"classifier",params:{in_features:64,out_features:10},isTraining:!1}};e.push(d);let c={id:(0,b.Ak)(6),type:"default",position:{x:1130,y:50},data:{registryKey:"torch.nn.CrossEntropyLoss",label:"loss",params:{reduction:"mean"},isTraining:!1}};e.push(c);let u=[{id:(0,b.Ak)(6),type:"default",position:{x:50,y:250},data:{registryKey:"transforms.RandomHorizontalFlip",label:"flip",params:{p:.5},isTraining:!0}},{id:(0,b.Ak)(6),type:"default",position:{x:230,y:250},data:{registryKey:"transforms.Normalize",label:"normalize",params:{mean:[.485,.456,.406],std:[.229,.224,.225]},isTraining:!0}}];t.push(...u);let p={id:(0,b.Ak)(6),type:"default",position:{x:410,y:250},data:{registryKey:"torch.optim.Adam",label:"optimizer",params:{lr:.001,weight_decay:1e-4},isTraining:!0}};t.push(p);let m={id:(0,b.Ak)(6),type:"default",position:{x:590,y:250},data:{registryKey:"metrics.Accuracy",label:"accuracy",params:{task:"multiclass",num_classes:10},isTraining:!0}};t.push(m);let h={id:(0,b.Ak)(6),type:"default",position:{x:770,y:250},data:{registryKey:"training.config",label:"config",params:{epochs:20,batch_size:64,device:"cuda"},isTraining:!0}};t.push(h);let f=[{id:"".concat(n.id,"-").concat(i.id),source:n.id,target:i.id,type:"default"},{id:"".concat(i.id,"-").concat(r.id),source:i.id,target:r.id,type:"default"},{id:"".concat(r.id,"-").concat(s.id),source:r.id,target:s.id,type:"default"},{id:"".concat(s.id,"-").concat(l.id),source:s.id,target:l.id,type:"default"},{id:"".concat(l.id,"-").concat(d.id),source:l.id,target:d.id,type:"default"},{id:"".concat(d.id,"-").concat(c.id),source:d.id,target:c.id,type:"default"}];a(a=>[...a,...e,...t]),o(e=>[...e,...f])},[a,o]),(0,i.useCallback)(()=>{let e=[],t=[],n={id:(0,b.Ak)(6),type:"default",position:{x:50,y:50},data:{registryKey:"input.dataset",label:"dataset",params:{dataset:"torchvision.datasets.CIFAR10"},isTraining:!1}};e.push(n);let i={id:(0,b.Ak)(6),type:"default",position:{x:250,y:50},data:{registryKey:"torch.nn.Conv2d",label:"conv1",params:{in_channels:3,out_channels:64,kernel_size:3,stride:1,padding:1},isTraining:!1}};e.push(i);let r={id:(0,b.Ak)(6),type:"default",position:{x:450,y:50},data:{registryKey:"blocks.InceptionModule",label:"inception1",params:{in_channels:64,out_1x1:32,out_3x3_reduce:48,out_3x3:64,out_5x5_reduce:8,out_5x5:16,out_pool:16},isTraining:!1}};e.push(r);let s={id:(0,b.Ak)(6),type:"default",position:{x:650,y:50},data:{registryKey:"torch.nn.AdaptiveAvgPool2d",label:"global_pool",params:{output_size:1},isTraining:!1}};e.push(s);let l={id:(0,b.Ak)(6),type:"default",position:{x:850,y:50},data:{registryKey:"torch.nn.Flatten",label:"flatten",params:{start_dim:1},isTraining:!1}};e.push(l);let d={id:(0,b.Ak)(6),type:"default",position:{x:1050,y:50},data:{registryKey:"torch.nn.Linear",label:"classifier",params:{in_features:128,out_features:10},isTraining:!1}};e.push(d);let c={id:(0,b.Ak)(6),type:"default",position:{x:1250,y:50},data:{registryKey:"torch.nn.CrossEntropyLoss",label:"loss",params:{reduction:"mean"},isTraining:!1}};e.push(c);let u=[{id:(0,b.Ak)(6),type:"default",position:{x:50,y:250},data:{registryKey:"transforms.RandomCrop",label:"crop",params:{size:32,padding:4},isTraining:!0}},{id:(0,b.Ak)(6),type:"default",position:{x:250,y:250},data:{registryKey:"transforms.ColorJitter",label:"jitter",params:{brightness:.2,contrast:.2,saturation:.2,hue:.1},isTraining:!0}}];t.push(...u);let p={id:(0,b.Ak)(6),type:"default",position:{x:450,y:250},data:{registryKey:"torch.optim.SGD",label:"optimizer",params:{lr:.01,momentum:.9,weight_decay:1e-4},isTraining:!0}};t.push(p);let m=[{id:(0,b.Ak)(6),type:"default",position:{x:650,y:250},data:{registryKey:"metrics.Accuracy",label:"accuracy",params:{task:"multiclass",num_classes:10},isTraining:!0}},{id:(0,b.Ak)(6),type:"default",position:{x:850,y:250},data:{registryKey:"metrics.F1Score",label:"f1",params:{task:"multiclass",num_classes:10,average:"macro"},isTraining:!0}}];t.push(...m);let h={id:(0,b.Ak)(6),type:"default",position:{x:1050,y:250},data:{registryKey:"training.config",label:"config",params:{epochs:30,batch_size:32,device:"cuda"},isTraining:!0}};t.push(h);let f=[{id:"".concat(n.id,"-").concat(i.id),source:n.id,target:i.id,type:"default"},{id:"".concat(i.id,"-").concat(r.id),source:i.id,target:r.id,type:"default"},{id:"".concat(r.id,"-").concat(s.id),source:r.id,target:s.id,type:"default"},{id:"".concat(s.id,"-").concat(l.id),source:s.id,target:l.id,type:"default"},{id:"".concat(l.id,"-").concat(d.id),source:l.id,target:d.id,type:"default"},{id:"".concat(d.id,"-").concat(c.id),source:d.id,target:c.id,type:"default"}];a(a=>[...a,...e,...t]),o(e=>[...e,...f])},[a,o]),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)("div",{className:"flex-1 h-screen bg-gray-100 relative",style:{zIndex:0},onDragOver:e=>e.preventDefault(),onDrop:B,children:["code"===l?(0,n.jsxs)("div",{className:"w-full h-full flex flex-col items-stretch bg-white overflow-hidden",children:[(0,n.jsxs)("div",{className:"flex justify-between items-center p-4 border-b",children:[(0,n.jsxs)("div",{className:"flex items-center gap-4",children:[(0,n.jsx)("h2",{className:"text-lg font-semibold text-gray-900",children:"Code Editor"}),(0,n.jsxs)("div",{className:"flex gap-2",children:[(0,n.jsx)("button",{onClick:()=>{f(Q()),A('Code generated successfully!\nClick "Run" to execute the code.')},className:"px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700 transition-colors",children:"Generate Code"}),(0,n.jsx)("button",{disabled:!0,className:"px-4 py-2 bg-gray-500 text-gray-300 rounded cursor-not-allowed",title:"Training execution temporarily disabled",children:"Run Training (Disabled)"})]})]}),(0,n.jsx)("div",{})]}),(0,n.jsxs)("div",{className:"flex-1 flex overflow-hidden",children:[(0,n.jsx)("div",{className:"flex-1 h-full",children:(0,n.jsx)(P,{code:h,onCodeChange:f})}),(0,n.jsxs)("div",{className:"w-1/3 border-l flex flex-col overflow-hidden",children:[(0,n.jsxs)("div",{className:"p-2 bg-gray-800 text-white font-semibold border-b flex justify-between items-center",children:[(0,n.jsx)("span",{children:"Console Output"}),(0,n.jsx)("button",{onClick:()=>A(""),className:"px-2 py-1 text-xs bg-gray-600 hover:bg-gray-500 rounded transition-colors",title:"Clear console",children:"Clear"})]}),(0,n.jsx)("div",{className:"flex-1 bg-gray-900 text-green-400 p-4 font-mono text-sm whitespace-pre-wrap overflow-y-auto",children:S||(0,n.jsxs)("div",{children:[(0,n.jsx)("div",{className:"text-gray-400 mb-2",children:"Console output will appear here..."}),(0,n.jsxs)("div",{className:"text-yellow-400 text-xs bg-gray-800 p-2 rounded border-l-4 border-yellow-500",children:["\uD83D\uDCCB ",(0,n.jsx)("strong",{children:"Note:"})," Training logs will appear when training completes (not real-time streaming)"]})]})})]})]})]}):(0,n.jsxs)(k.Gc,{nodes:$,edges:O,onNodesChange:e=>a(t=>(0,k._0)(e,t)),onEdgesChange:e=>o(t=>(0,k.zW)(e,t)),onConnect:G,onConnectStart:J,onConnectEnd:X,onNodeClick:Z,onEdgeClick:Y,onPaneClick:()=>{_(null),j(null)},defaultViewport:{x:0,y:0,zoom:1},nodeTypes:U,edgeTypes:W,children:[(0,n.jsx)("svg",{style:{position:"absolute",width:0,height:0},children:(0,n.jsxs)("defs",{children:[(0,n.jsx)("marker",{id:"arrowhead",markerWidth:"25",markerHeight:"25",viewBox:"-12 -12 24 24",orient:"auto",refX:"8",refY:"0",markerUnits:"strokeWidth",children:(0,n.jsx)("path",{stroke:"#1e40af",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round",fill:"#1e40af",d:"M-8,-6 L0,0 L-8,6 L-5,0 z"})}),(0,n.jsx)("marker",{id:"residual-arrow",markerWidth:"25",markerHeight:"25",viewBox:"-12 -12 24 24",orient:"auto",refX:"8",refY:"0",markerUnits:"strokeWidth",children:(0,n.jsx)("path",{stroke:"#059669",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round",fill:"#059669",d:"M-8,-6 L0,0 L-8,6 L-5,0 z"})}),(0,n.jsxs)("marker",{id:"sum-arrow",markerWidth:"25",markerHeight:"25",viewBox:"-12 -12 24 24",orient:"auto",refX:"8",refY:"0",markerUnits:"strokeWidth",children:[(0,n.jsx)("circle",{cx:"0",cy:"0",r:"6",stroke:"#dc2626",strokeWidth:"2",fill:"#fef2f2"}),(0,n.jsx)("text",{x:"0",y:"2",textAnchor:"middle",fontSize:"8",fontWeight:"bold",fill:"#dc2626",children:"+"})]})]})}),(0,n.jsx)(w.o,{}),(0,n.jsx)(N.H,{}),(0,n.jsx)(C.V,{gap:16})]}),(0,n.jsxs)("div",{className:"fixed bottom-0 left-0 w-full flex justify-center items-center bg-white border-t py-2 z-20 shadow gap-2",children:["model"===l&&(0,n.jsxs)("div",{className:"flex items-center gap-2 mr-4",children:[(0,n.jsx)("span",{className:"text-sm font-medium text-gray-900",children:T?"Selected Edge Type:":"Edge Type:"}),(0,n.jsxs)("select",{value:q,onChange:e=>{let t=e.target.value;V(t),T&&o(e=>e.map(e=>e.id===T?{...e,type:t}:e))},className:"text-sm border border-gray-300 rounded px-2 py-1 bg-white text-gray-900 ".concat(T?"ring-2 ring-blue-200 border-blue-300":""),children:[(0,n.jsx)("option",{value:"default",children:"Default"}),(0,n.jsx)("option",{value:"residual",children:"Residual"}),(0,n.jsx)("option",{value:"sum",children:"Sum"})]})]}),(0,n.jsx)("button",{className:"mx-2 p-2 rounded-full border-2 ".concat("model"===l?"bg-blue-100 border-blue-600":"bg-gray-100 border-gray-300"," transition-colors"),onClick:()=>{d("model"),"model"!==l&&V("default")},title:"Model Mode",children:(0,n.jsx)(z.yd7,{className:"model"===l?"text-blue-700":"text-gray-700"})}),(0,n.jsx)("button",{className:"mx-2 p-2 rounded-full border-2 ".concat("training"===l?"bg-blue-100 border-blue-600":"bg-gray-100 border-gray-300"," transition-colors ").concat(H?"":"opacity-50 cursor-not-allowed"),onClick:()=>{H&&(d("training"),V("default"))},title:H?"Training Mode":"Add a dataset node first",disabled:!H,children:(0,n.jsx)(z.Pcn,{className:"training"===l?"text-blue-700":"text-gray-700"})}),(0,n.jsx)("button",{className:"mx-2 p-2 rounded-full border-2 ".concat("code"===l?"bg-blue-100 border-blue-600":"bg-gray-100 border-gray-300"," transition-colors"),onClick:()=>{d("code"),"model"===l&&V("default")},title:"Code Mode",children:(0,n.jsx)(z.FSj,{className:"code"===l?"text-blue-700":"text-gray-700"})})]})]}),x&&"code"!==l&&(0,n.jsx)(L,{nodeId:x,onClose:()=>_(null)})]})}function B(){return(0,n.jsx)(k.Ln,{children:(0,n.jsx)(H,{})})}var O=a(1482),G=a(7039);function J(e){let{isOpen:t,onClose:a,onContinueWithoutLogin:r}=e,[o,s]=(0,i.useState)("login"),[l,d]=(0,i.useState)(""),[c,u]=(0,i.useState)(""),[p,m]=(0,i.useState)(""),[h,f]=(0,i.useState)(""),[g,y]=(0,i.useState)(""),[b,v]=(0,i.useState)(!1),[x,_]=(0,i.useState)(!1),{setAuth:k,setTokens:w}=(0,M.n)(),N=e=>{let t=[{test:e.length>=8,text:"At least 8 characters"},{test:/[A-Z]/.test(e),text:"One uppercase letter"},{test:/[a-z]/.test(e),text:"One lowercase letter"},{test:/\d/.test(e),text:"One number"},{test:/[!@#$%^&*(),.?":{}|<>]/.test(e),text:"One special character"},{test:!/(.)\1{2,}/.test(e),text:"No repeating characters (aaa)"},{test:!/(012|123|234|345|456|567|678|789|890|abc|bcd|cde|def)/i.test(e),text:"No sequential patterns (123, abc)"}],a=t.filter(e=>e.test).length/t.length;return{checks:t,strength:a,color:a<.5?"text-red-500":a<.8?"text-yellow-500":"text-green-500",bgColor:a<.5?"bg-red-200":a<.8?"bg-yellow-200":"bg-green-200"}};(0,i.useEffect)(()=>{t||(d(""),u(""),m(""),f(""),y(""),_(!1))},[t]);let C=async e=>{e.preventDefault(),y(""),v(!0);try{let e=await G.FH.login(l,p);w(e.access_token,e.refresh_token);let t=await G.FH.getCurrentUser();k(t,e.access_token,e.refresh_token),a()}catch(e){y(e.detail||"Login failed")}finally{v(!1)}},T=async e=>{if(e.preventDefault(),y(""),p!==h)return void y("Passwords do not match");if(p.length<8)return void y("Password must be at least 8 characters long");if(!/[A-Z]/.test(p))return void y("Password must contain at least one uppercase letter");if(!/[a-z]/.test(p))return void y("Password must contain at least one lowercase letter");if(!/\d/.test(p))return void y("Password must contain at least one number");if(!/[!@#$%^&*(),.?":{}|<>]/.test(p))return void y("Password must contain at least one special character (!@#$%^&* etc.)");if(/(.)\1{2,}/.test(p))return void y("Password cannot contain three consecutive identical characters");if(/(012|123|234|345|456|567|678|789|890|abc|bcd|cde|def)/i.test(p))return void y("Password cannot contain common sequential patterns (like 123 or abc)");if(["password","123456","password123","admin","qwerty","letmein","welcome","monkey"].includes(p.toLowerCase()))return void y("This password is too common. Please choose a more unique password.");v(!0);try{await G.FH.register(l,c,p),_(!0)}catch(e){console.error("Registration error:",e),y(e.message||e.detail||"Registration failed")}finally{v(!1)}};return t?(0,n.jsx)("div",{className:"fixed inset-0 flex items-center justify-center z-50",style:{backgroundColor:"rgba(0, 0, 0, 0.35)"},children:(0,n.jsxs)("div",{className:"bg-white rounded-lg max-w-md w-full p-6 relative",children:[(0,n.jsx)("button",{onClick:a,className:"absolute top-4 right-4 text-gray-400 hover:text-gray-600",children:(0,n.jsx)(O.A,{size:24})}),x?(0,n.jsxs)("div",{className:"text-center py-8",children:[(0,n.jsx)("h2",{className:"text-2xl font-bold mb-4",children:"Check Your Email"}),(0,n.jsxs)("p",{className:"text-gray-600 mb-6",children:["We've sent a verification email to ",(0,n.jsx)("strong",{children:l}),". Please check your inbox and click the link to verify your account."]}),(0,n.jsx)("button",{onClick:a,className:"bg-blue-600 text-white px-6 py-2 rounded hover:bg-blue-700",children:"OK"})]}):(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("h2",{className:"text-2xl font-bold mb-6 text-gray-900",children:"login"===o?"Welcome Back":"Create Account"}),(0,n.jsx)("form",{onSubmit:"login"===o?C:T,children:(0,n.jsxs)("div",{className:"space-y-4",children:[(0,n.jsxs)("div",{children:[(0,n.jsx)("label",{className:"block text-sm font-medium mb-1 text-gray-900",children:"Email"}),(0,n.jsx)("input",{type:"email",value:l,onChange:e=>d(e.target.value),className:"w-full px-3 py-2 border rounded focus:outline-none focus:ring-2 focus:ring-blue-500 text-gray-900",required:!0})]}),"signup"===o&&(0,n.jsxs)("div",{children:[(0,n.jsx)("label",{className:"block text-sm font-medium mb-1 text-gray-900",children:"Username"}),(0,n.jsx)("input",{type:"text",value:c,onChange:e=>u(e.target.value),className:"w-full px-3 py-2 border rounded focus:outline-none focus:ring-2 focus:ring-blue-500 text-gray-900",required:!0})]}),(0,n.jsxs)("div",{children:[(0,n.jsx)("label",{className:"block text-sm font-medium mb-1 text-gray-900",children:"Password"}),(0,n.jsx)("input",{type:"password",value:p,onChange:e=>m(e.target.value),className:"w-full px-3 py-2 border rounded focus:outline-none focus:ring-2 focus:ring-blue-500 text-gray-900",required:!0}),"signup"===o&&p&&(0,n.jsxs)("div",{className:"mt-2",children:[(0,n.jsx)("div",{className:"text-xs text-gray-600 mb-1",children:"Password strength:"}),(0,n.jsx)("div",{className:"w-full bg-gray-200 rounded-full h-2 mb-2",children:(0,n.jsx)("div",{className:"h-2 rounded-full transition-all duration-300 ".concat(N(p).bgColor),style:{width:"".concat(100*N(p).strength,"%")}})}),(0,n.jsx)("div",{className:"text-xs space-y-1",children:N(p).checks.map((e,t)=>(0,n.jsxs)("div",{className:"flex items-center ".concat(e.test?"text-green-600":"text-gray-400"),children:[(0,n.jsx)("span",{className:"mr-1",children:e.test?"✓":"○"}),e.text]},t))})]})]}),"signup"===o&&(0,n.jsxs)("div",{children:[(0,n.jsx)("label",{className:"block text-sm font-medium mb-1 text-gray-900",children:"Confirm Password"}),(0,n.jsx)("input",{type:"password",value:h,onChange:e=>f(e.target.value),className:"w-full px-3 py-2 border rounded focus:outline-none focus:ring-2 focus:ring-blue-500 text-gray-900",required:!0})]}),g&&(0,n.jsx)("div",{className:"text-red-600 text-sm",children:String(g)}),(0,n.jsx)("button",{type:"submit",disabled:b,className:"w-full bg-blue-600 text-white py-2 rounded hover:bg-blue-700 disabled:opacity-50",children:b?"Loading...":"login"===o?"Login":"Sign Up"})]})}),(0,n.jsx)("div",{className:"mt-6 text-center",children:(0,n.jsxs)("p",{className:"text-sm text-gray-800",children:["login"===o?"Don't have an account? ":"Already have an account? ",(0,n.jsx)("button",{onClick:()=>{s("login"===o?"signup":"login"),y("")},className:"text-blue-600 hover:underline",children:"login"===o?"Sign up":"Log in"})]})}),(0,n.jsxs)("div",{className:"mt-6 pt-6 border-t",children:[(0,n.jsx)("button",{onClick:r,className:"w-full text-gray-800 py-2 border rounded hover:bg-gray-50",children:"Continue Without Account"}),(0,n.jsx)("p",{className:"text-xs text-gray-500 text-center mt-2",children:"You won't be able to save your models online"})]})]})]})}):null}var X=a(5285),$=a(232),Y=a(2267),Z=a(2065);function Q(){let{user:e,logout:t}=(0,M.n)(),[a,r]=(0,i.useState)(!1);return e?(0,n.jsxs)("div",{className:"relative",children:[(0,n.jsxs)("button",{onClick:()=>r(!a),className:"flex items-center gap-2 bg-white rounded-lg shadow px-4 py-2 text-sm hover:shadow-md transition-shadow",children:[(0,n.jsx)(X.A,{size:16,className:"text-gray-900"}),(0,n.jsx)("span",{className:"font-semibold text-gray-900",children:e.username}),(0,n.jsx)($.A,{size:16,className:"transition-transform text-gray-900 ".concat(a?"rotate-180":"")})]}),a&&(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)("div",{className:"fixed inset-0 z-10",onClick:()=>r(!1)}),(0,n.jsxs)("div",{className:"absolute right-0 mt-2 w-48 bg-white rounded-lg shadow-lg py-1 z-20",children:[(0,n.jsxs)("div",{className:"px-4 py-2 border-b",children:[(0,n.jsx)("p",{className:"text-xs text-gray-600",children:"Signed in as"}),(0,n.jsx)("p",{className:"text-sm font-medium truncate text-gray-900",children:e.email})]}),(0,n.jsxs)("button",{onClick:()=>{r(!1)},className:"w-full text-left px-4 py-2 text-sm hover:bg-gray-100 flex items-center gap-2 text-gray-900",children:[(0,n.jsx)(Y.A,{size:16}),"Saved Models"]}),(0,n.jsx)("hr",{className:"my-1"}),(0,n.jsxs)("button",{onClick:()=>{t(),r(!1)},className:"w-full text-left px-4 py-2 text-sm hover:bg-gray-100 text-red-700 flex items-center gap-2 font-medium",children:[(0,n.jsx)(Z.A,{size:16}),"Logout"]})]})]})]}):null}function ee(){let{mode:e}=y(),{isAuthenticated:t,user:a}=(0,M.n)(),[r,o]=(0,i.useState)(!1),[s,l]=(0,i.useState)(!1);return(0,i.useEffect)(()=>{s||t||setTimeout(()=>{o(!0),l(!0)},1e3)},[s,t]),(0,i.useEffect)(()=>{if(!t)return;let e=async()=>{let{nodes:e,edges:t}=y.getState();try{await G.FH.autosaveModel({nodes:e,edges:t})}catch(e){console.error("Autosave failed:",e)}},a=setInterval(e,3e4),n=y.subscribe(t=>{clearTimeout(window.autosaveTimeout),window.autosaveTimeout=setTimeout(e,2e3)});return()=>{clearInterval(a),n(),clearTimeout(window.autosaveTimeout)}},[t]),(0,n.jsxs)("div",{className:"flex h-screen bg-gray-100",children:["code"!==e&&(0,n.jsx)(_,{}),(0,n.jsx)(B,{}),(0,n.jsx)(J,{isOpen:r,onClose:()=>o(!1),onContinueWithoutLogin:()=>o(!1)}),a&&(0,n.jsx)("div",{className:"absolute top-4 right-4 z-50",children:(0,n.jsx)(Q,{})})]})}},5839:(e,t,a)=>{Promise.resolve().then(a.bind(a,2480))},7039:(e,t,a)=>{"use strict";a.d(t,{FH:()=>l});var n=a(2323);let i=a(7946).env.NEXT_PUBLIC_API_URL||"http://localhost:8000";class r{static sanitizeInput(e){if("string"==typeof e)return e.replace(/[<>'"]/g,"").trim().slice(0,1e4);if(Array.isArray(e))return e.map(e=>r.sanitizeInput(e));if("object"==typeof e&&null!==e){let t={};for(let[a,n]of Object.entries(e))/^[a-zA-Z0-9_-]+$/.test(a)&&(t[a]=r.sanitizeInput(n));return t}return e}static isValidEmail(e){return/^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(e)&&e.length<=255}static validatePassword(e){let t=[];return e.length<8&&t.push("Password must be at least 8 characters long"),/[A-Z]/.test(e)||t.push("Password must contain at least one uppercase letter"),/[a-z]/.test(e)||t.push("Password must contain at least one lowercase letter"),/\d/.test(e)||t.push("Password must contain at least one number"),/[!@#$%^&*(),.?":{}|<>]/.test(e)||t.push("Password must contain at least one special character"),{isValid:0===t.length,errors:t}}static generateCSRFToken(){let e=new Uint8Array(32);return crypto.getRandomValues(e),Array.from(e,e=>e.toString(16).padStart(2,"0")).join("")}static isValidContentType(e){return["application/json","text/plain","multipart/form-data"].some(t=>e.includes(t))}}class o{async checkRateLimit(){let e=Date.now();if(e-this.state.windowStart>=this.windowMs&&(this.state.windowStart=e,this.state.requestCount=0),this.state.requestCount>=this.maxRequests){let t=this.windowMs-(e-this.state.windowStart);throw Error("Rate limit exceeded. Please wait ".concat(Math.ceil(t/1e3)," seconds."))}let t=e-this.state.lastRequest;t<1e3&&await new Promise(e=>setTimeout(e,1e3-t)),this.state.requestCount++,this.state.lastRequest=e}constructor(){this.state={lastRequest:0,requestCount:0,windowStart:Date.now()},this.maxRequests=60,this.windowMs=6e4}}class s{async refreshTokens(){let{refreshToken:e,updateTokens:t,logout:a}=n.n.getState();if(!e)return console.log("No refresh token available"),!1;console.log("Attempting to refresh tokens...");try{let n=await fetch("".concat(this.baseURL,"/auth/refresh"),{method:"POST",headers:{"Content-Type":"application/json","X-CSRF-Token":this.csrfToken},body:JSON.stringify({refresh_token:e})});if(console.log("Token refresh response:",n.status,n.statusText),n.ok){let e=n.headers.get("content-type")||"";if(!r.isValidContentType(e))return console.error("Invalid content type in refresh response"),!1;let a=await n.json();return console.log("Token refresh successful, updating tokens"),t(a.access_token,a.refresh_token),!0}{console.log("Token refresh failed with status:",n.status);let e=await n.json().catch(()=>({}));console.log("Token refresh error details:",e),a()}}catch(e){console.error("Token refresh failed:",e),a()}return!1}async makeRequest(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},a=arguments.length>2&&void 0!==arguments[2]&&arguments[2];await this.rateLimiter.checkRateLimit();let{accessToken:i,logout:o}=n.n.getState(),s=new Headers(t.headers);s.set("Content-Type","application/json"),s.set("X-CSRF-Token",this.csrfToken),s.set("X-Requested-With","XMLHttpRequest"),i?(s.set("Authorization","Bearer ".concat(i)),console.log("Making request to ".concat(e," with token:"),i.substring(0,20)+"...")):console.log("Making request to ".concat(e," without token"));let l=new AbortController,d=setTimeout(()=>l.abort(),1e4);try{let n=await fetch("".concat(this.baseURL).concat(e),{...t,headers:s,signal:l.signal,credentials:this.baseURL.includes(window.location.hostname)?"include":"omit"});clearTimeout(d),console.log("Response from ".concat(e,":"),n.status,n.statusText);let c=n.headers.get("content-type")||"";if(n.ok&&!r.isValidContentType(c))throw Error("Invalid response content type");if(401===n.status&&!a&&i){if(console.log("401 error detected, attempting token refresh..."),await this.refreshTokens())return console.log("Token refresh successful, retrying request..."),this.makeRequest(e,t,!0);console.log("Token refresh failed, logging out..."),o()}return n}catch(t){if(clearTimeout(d),t instanceof Error&&"AbortError"===t.name)throw console.log("Request to ".concat(e," timed out after ").concat(1e4,"ms")),Error("Request timeout: ".concat(e));throw console.log("Request to ".concat(e," failed:"),t),t}}async get(e){let t=await this.makeRequest(e);if(!t.ok)throw await this.handleError(t);return t.json()}async post(e,t){let a=t?r.sanitizeInput(t):void 0,n=await this.makeRequest(e,{method:"POST",body:a?JSON.stringify(a):void 0});if(!n.ok)throw await this.handleError(n);return n.json()}async put(e,t){let a=t?r.sanitizeInput(t):void 0,n=await this.makeRequest(e,{method:"PUT",body:a?JSON.stringify(a):void 0});if(!n.ok)throw await this.handleError(n);return n.json()}async delete(e){let t=await this.makeRequest(e,{method:"DELETE"});if(!t.ok)throw await this.handleError(t);return t.json()}async handleError(e){let t="An error occurred";try{let a=e.headers.get("content-type")||"";if(r.isValidContentType(a)){let a=await e.json();t=a.detail||a.message||t}}catch(a){t="HTTP ".concat(e.status,": ").concat(e.statusText)}return{detail:t,status:e.status}}async login(e,t){if(!r.isValidEmail(e))throw Error("Invalid email format");if(!t||t.length<8)throw Error("Invalid password");let a=new FormData;a.append("username",e),a.append("password",t);let n=new Headers;n.set("X-CSRF-Token",this.csrfToken),n.set("X-Requested-With","XMLHttpRequest");let i=await fetch("".concat(this.baseURL,"/auth/login"),{method:"POST",headers:n,body:a,credentials:"include"});if(!i.ok)throw await this.handleError(i);return i.json()}async register(e,t,a){if(!r.isValidEmail(e))throw Error("Invalid email format");if(!t||t.length<3||t.length>50)throw Error("Username must be between 3 and 50 characters");let n=r.validatePassword(a);if(!n.isValid)throw Error(n.errors.join("; "));let i={email:e.toLowerCase().trim(),username:t.trim(),password:a},o=await fetch("".concat(this.baseURL,"/auth/register"),{method:"POST",headers:{"Content-Type":"application/json","X-CSRF-Token":this.csrfToken,"X-Requested-With":"XMLHttpRequest"},body:JSON.stringify(i),credentials:"include"});if(!o.ok)throw await this.handleError(o);return o.json()}async verifyEmail(e){if(!/^[A-Za-z0-9_-]+$/.test(e)||e.length<20)throw Error("Invalid verification token format");return this.post("/auth/verify-email",{token:e})}async getCurrentUser(){return this.get("/auth/me")}async createModel(e){if(!e.name||"string"!=typeof e.name)throw Error("Model name is required");if(e.name.length>255)throw Error("Model name too long");return this.post("/models",e)}async getModels(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:0,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:20;return e=Math.max(0,Math.min(e,1e4)),t=Math.max(1,Math.min(t,100)),this.get("/models?skip=".concat(e,"&limit=").concat(t))}async getModel(e){if(!/^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(e))throw Error("Invalid model ID format");return this.get("/models/".concat(e))}async updateModel(e,t){if(!/^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(e))throw Error("Invalid model ID format");return this.put("/models/".concat(e),t)}async deleteModel(e){if(!/^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(e))throw Error("Invalid model ID format");return this.delete("/models/".concat(e))}async autosaveModel(e){return this.post("/models/autosave",e)}async generateCode(e){if(!/^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(e))throw Error("Invalid model ID format");return this.post("/models/".concat(e,"/generate-code"),{})}constructor(e){this.baseURL=e,this.rateLimiter=new o,this.csrfToken=r.generateCSRFToken()}}let l=new s(i)}},e=>{var t=t=>e(e.s=t);e.O(0,[83,458,783,551,309,951,358],()=>t(5839)),_N_E=e.O()}]);